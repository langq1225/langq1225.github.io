---
title: "arXiv è®ºæ–‡æ·±åº¦è§£è¯» â€” 2026 å¹´ 2 æœˆ 23 æ—¥"
date: 2026-02-23
draft: false
description: "Diffusion LM å‰ªæã€Efficient AI æœ€æ–°æŠ€æœ¯ç»†èŠ‚åˆ†æ"
tags: ["arxiv", "efficient-ai", "diffusion-lm", "pruning", "quantization"]
---

# arXiv è®ºæ–‡æ·±åº¦è§£è¯» â€” 2026 å¹´ 2 æœˆ 23 æ—¥

> ğŸ”¬ æŠ€æœ¯ç»†èŠ‚ + ä»£ç çº§åˆ†æ + å¯¹ä½ çš„ç ”ç©¶ä»·å€¼

---

## ğŸ“Š æœ¬æœŸè®ºæ–‡æ¸…å•

| è®ºæ–‡ | æ–¹å‘ | å¯¹ä½ çš„ä»·å€¼ |
|------|------|-----------|
| **Sink-Aware Pruning for DLMs** | å‰ªæ | â­â­â­â­â­ ç›´æ¥å¯ç”¨ |
| **Fast Analytical Diffusion** | åŠ é€Ÿé‡‡æ · | â­â­â­â­ æ€è·¯å€Ÿé‰´ |
| **Hardware-Aware DNN Compression** | é‡åŒ– + å‰ªæ | â­â­â­â­ ç¡¬ä»¶ååŒ |

---

## 1ï¸âƒ£ Sink-Aware Pruning for Diffusion Language Models

**ğŸ“„ arXiv:** [2602.17664](https://arxiv.org/abs/2602.17664)  
**ğŸ›ï¸ æœºæ„:** VILA Lab, MBZUAI  
**ğŸ“… å‘å¸ƒ:** 3 å¤©å‰ï¼ˆ2026-02-20ï¼‰  
**ğŸ’» ä»£ç :** [GitHub - Sink-Aware-Pruning](https://github.com/VILA-Lab/Sink-Aware-Pruning)

---

### ğŸ¯ æ ¸å¿ƒé—®é¢˜

**Diffusion Language Models (DLMs) æ¨ç†æˆæœ¬é«˜ï¼š**
- éœ€è¦å¤šæ¬¡è¿­ä»£å»å™ªï¼ˆé€šå¸¸ 10-100 æ­¥ï¼‰
- æ¯æ­¥éƒ½è¦è®¡ç®—å®Œæ•´åºåˆ—çš„ attention
- æ¯” Autoregressive (AR) æ¨¡å‹æ…¢ 10-100 å€

**ç°æœ‰å‰ªææ–¹æ³•çš„é—®é¢˜ï¼š**
- ç›´æ¥å¥—ç”¨ AR LLM çš„å‰ªæç­–ç•¥
- **å‡è®¾ï¼š** attention sink tokens å¿…é¡»ä¿ç•™
- **ä½†ï¼š** DLM çš„ attention åŠ¨æ€ä¸ AR å®Œå…¨ä¸åŒ

---

### ğŸ”¬ å…³é”®å‘ç°ï¼šDLM çš„ Sink æ˜¯ä¸ç¨³å®šçš„

#### AR vs DLM çš„ Sink è¡Œä¸ºå¯¹æ¯”

**AR æ¨¡å‹ï¼ˆå¦‚ LLaMAï¼‰ï¼š**
```
Step 1:  [BOS] â†’ sink at position 0
Step 2:  [BOS, tok1] â†’ sink at position 0
Step 3:  [BOS, tok1, tok2] â†’ sink at position 0
...
Step N:  [BOS, tok1, ..., tokN] â†’ sink at position 0
```
**ç‰¹ç‚¹ï¼š** sink ä½ç½®å›ºå®šï¼ˆé€šå¸¸æ˜¯ BOS æˆ–å‰ç¼€ tokenï¼‰

**DLM æ¨¡å‹ï¼ˆå¦‚ LLaDAï¼‰ï¼š**
```
Step 1 (25%):  sink at positions [0, 5, 12]
Step 2 (50%):  sink at positions [0, 8, 15]
Step 3 (75%):  sink at positions [0, 3, 20]
```
**ç‰¹ç‚¹ï¼š** sink ä½ç½®éšå»å™ªæ­¥éª¤å˜åŒ–

---

### ğŸ“ æŠ€æœ¯ç»†èŠ‚ï¼šSink Variance åº¦é‡

**å®šä¹‰ï¼š**
```
Sink Variance = Var({sink_position_t | t âˆˆ [1, T]})
```

**è®¡ç®—æ–¹æ³•ï¼š**
1. å¯¹æ¯ä¸ª diffusion timestep `t`ï¼Œè®¡ç®— attention map
2. æ‰¾å‡º attention mass æœ€å¤§çš„ top-k ä½ç½®ï¼ˆsinksï¼‰
3. è¿½è¸ªè¿™äº›ä½ç½®åœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­çš„å˜åŒ–
4. è®¡ç®—æ–¹å·®

**è®ºæ–‡ä¸­çš„å…³é”®å›¾è¡¨ï¼š**

```
Figure 2: Attention sink heatmap

AR LLM (LLaMA-3-8B):
     Step 25%  â”‚  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  â† sink å›ºå®šåœ¨ position 0
     Step 50%  â”‚  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘
     Step 75%  â”‚  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘

DLM (LLaDA):
     Step 25%  â”‚  â–ˆâ–‘â–‘â–‘â–ˆâ–‘â–‘â–‘  â† sink åœ¨å˜åŒ–
     Step 50%  â”‚  â–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–‘
     Step 75%  â”‚  â–ˆâ–‘â–ˆâ–‘â–‘â–‘â–‘â–‘
```

---

### ğŸ› ï¸ æ–¹æ³•ï¼šSink-Aware Pruning

#### ç®—æ³•æµç¨‹

```python
# ä¼ªä»£ç ï¼ˆåŸºäºè®ºæ–‡æè¿°ï¼‰

def sink_aware_pruning(model, inputs, n_steps=50):
    # 1. æ”¶é›†æ•´ä¸ªå»å™ªè½¨è¿¹çš„ attention ç»Ÿè®¡
    sink_positions = []
    for t in range(n_steps):
        attn_map = model.get_attention(inputs, timestep=t)
        sinks = find_top_k_sinks(attn_map, k=5)
        sink_positions.append(sinks)
    
    # 2. è®¡ç®—æ¯ä¸ªä½ç½®çš„ sink variance
    variance_per_token = {}
    for pos in range(seq_len):
        # è®¡ç®—è¯¥ä½ç½®ä½œä¸º sink çš„é¢‘ç‡
        sink_frequency = sum(1 for sinks in sink_positions if pos in sinks)
        # è®¡ç®—æ–¹å·®ï¼ˆé¢‘ç‡è¶Šä½ï¼Œæ–¹å·®è¶Šé«˜ï¼‰
        variance_per_token[pos] = 1.0 - (sink_frequency / n_steps)
    
    # 3. å‰ªæï¼šä¿ç•™ä½æ–¹å·®ï¼ˆç¨³å®šï¼‰çš„ sinkï¼Œå‰ªæ‰é«˜æ–¹å·®ï¼ˆä¸ç¨³å®šï¼‰çš„
    prune_mask = {}
    for layer in model.layers:
        for head in layer.attention_heads:
            for pos in range(seq_len):
                if variance_per_token[pos] > threshold:
                    prune_mask[(layer, head, pos)] = True  # å‰ªæ‰
    
    # 4. åº”ç”¨å‰ªæ
    model.apply_pruning(prune_mask)
    return model
```

---

### ğŸ“Š å®éªŒç»“æœ

#### æ€§èƒ½å¯¹æ¯”ï¼ˆåœ¨ GSM8K ä¸Šï¼‰

| æ–¹æ³• | å‰ªæç‡ | å‡†ç¡®ç‡ | åŠ é€Ÿæ¯” |
|------|--------|--------|--------|
| **Baseline (æ— å‰ªæ)** | 0% | 78.5% | 1.0x |
| **Random Pruning** | 30% | 65.2% | 1.4x |
| **Magnitude Pruning** | 30% | 70.1% | 1.4x |
| **AR Sink-Preserve** | 30% | 72.3% | 1.4x |
| **Sink-Aware (Ours)** | 30% | **75.8%** | 1.4x |
| **Sink-Aware (Ours)** | 50% | **73.2%** | 1.8x |

**å…³é”®ç»“è®ºï¼š**
- åœ¨ç›¸åŒå‰ªæç‡ä¸‹ï¼ŒSink-Aware æ¯” AR æ–¹æ³•é«˜ 3.5% å‡†ç¡®ç‡
- å¯ä»¥å‰ªåˆ° 50% ä»ä¿æŒ 73% å‡†ç¡®ç‡ï¼ˆAR æ–¹æ³•ä¼šå´©æºƒï¼‰

---

### ğŸ’¡ å¯¹ä½ çš„ç ”ç©¶ä»·å€¼

#### 1. **ç›´æ¥å¯ç”¨çš„å‰ªæç­–ç•¥**

å¦‚æœä½ åœ¨åš **Diffusion LM çš„æ•ˆç‡ä¼˜åŒ–**ï¼š

```python
# å¯ä»¥ç›´æ¥å€Ÿé‰´çš„ä»£ç æ€è·¯

import torch

def compute_sink_variance(attention_maps):
    """
    attention_maps: List[Tensor], shape [batch, heads, seq_len, seq_len]
    è¿”å›ï¼šæ¯ä¸ª token ä½ç½®ä½œä¸º sink çš„æ–¹å·®
    """
    n_steps = len(attention_maps)
    seq_len = attention_maps[0].shape[-1]
    
    # å¯¹æ¯ä¸ªä½ç½®ï¼Œè®¡ç®—å®ƒä½œä¸º sink çš„é¢‘ç‡
    sink_counts = torch.zeros(seq_len)
    for attn in attention_maps:
        # å¯¹æ¯ä¸ª headï¼Œæ‰¾å‡º attention mass æœ€å¤§çš„ä½ç½®
        attn_sum = attn.mean(dim=1)  # [batch, seq_len, seq_len]
        sinks = attn_sum.argmax(dim=-1)  # [batch, seq_len]
        for pos in sinks.unique():
            sink_counts[pos] += 1
    
    # å½’ä¸€åŒ–å¾—åˆ°é¢‘ç‡
    sink_frequency = sink_counts / n_steps
    
    # æ–¹å·® = 1 - é¢‘ç‡ï¼ˆé¢‘ç‡è¶Šä½ï¼Œæ–¹å·®è¶Šé«˜ï¼‰
    variance = 1.0 - sink_frequency
    return variance
```

---

#### 2. **å¯ä»¥æ‰©å±•çš„ç ”ç©¶æ–¹å‘**

**æ–¹å‘ Aï¼šTimestep-Adaptive Pruning**
```
ä¸åŒ diffusion timestep ä½¿ç”¨ä¸åŒçš„å‰ªæç­–ç•¥ï¼š
- æ—©æœŸï¼ˆé«˜å™ªå£°ï¼‰ï¼šä¿ç•™æ›´å¤š attentionï¼Œéœ€è¦å…¨å±€ç»“æ„
- åæœŸï¼ˆä½å™ªå£°ï¼‰ï¼šæ¿€è¿›å‰ªæï¼Œåªä¿ç•™å±€éƒ¨ç»†èŠ‚
```

**æ–¹å‘ Bï¼šLayer-Wise Sink Policy**
```
ä¸åŒå±‚çš„ sink è¡Œä¸ºå¯èƒ½ä¸åŒï¼š
- æµ…å±‚ï¼šsink å˜åŒ–å¤§ï¼ˆå¤„ç†å…¨å±€ç»“æ„ï¼‰
- æ·±å±‚ï¼šsink æ›´ç¨³å®šï¼ˆå¤„ç†è¯­ä¹‰ç»†èŠ‚ï¼‰
â†’ å¯ä»¥åˆ†å±‚è®¾ç½®å‰ªæé˜ˆå€¼
```

**æ–¹å‘ Cï¼šJoint Pruning + Quantization**
```
è®ºæ–‡æåˆ°ï¼š"Future work can explore joint optimization with quantization"
â†’ ä½ å¯ä»¥åšï¼šåŒæ—¶ä¼˜åŒ–å‰ªæå’Œé‡åŒ–ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„ quality-efficiency frontier
```

---

#### 3. **å®éªŒå»ºè®®**

**å¿«é€ŸéªŒè¯ï¼ˆ1-2 å¤©ï¼‰ï¼š**
1. ä¸‹è½½ LLaDA æˆ–ç±»ä¼¼ DLM æ¨¡å‹
2. å®ç° sink variance è®¡ç®—
3. å¯è§†åŒ– sink ä½ç½®å˜åŒ–ï¼ˆéªŒè¯è®ºæ–‡ç»“è®ºï¼‰

**ä¸­ç­‰å®éªŒï¼ˆ1-2 å‘¨ï¼‰ï¼š**
1. å®ç° Sink-Aware Pruning
2. åœ¨ GSM8K æˆ–å…¶ä»–åŸºå‡†ä¸Šæµ‹è¯•
3. å¯¹æ¯” AR å‰ªææ–¹æ³•

**æ·±å…¥ç ”ç©¶ï¼ˆ1-2 æœˆï¼‰ï¼š**
1. æ‰©å±•åˆ°æ—¶åºè‡ªé€‚åº”å‰ªæ
2. ç»“åˆé‡åŒ–ï¼ˆINT8/INT4ï¼‰
3. å†™è®ºæ–‡

---

### ğŸ”— ç›¸å…³èµ„æº

- **ä»£ç åº“ï¼š** https://github.com/VILA-Lab/Sink-Aware-Pruning
- **AR å‰ªæåŸºçº¿ï¼š** https://github.com/mit-han-lab/llm-awq
- **DLM æ¨¡å‹ï¼š** https://github.com/LLaDA-V/LLaDA

---

## 2ï¸âƒ£ Fast and Scalable Analytical Diffusion

**ğŸ“„ arXiv:** [2602.16498](https://arxiv.org/abs/2602.16498)  
**ğŸ“… å‘å¸ƒ:** 4 å¤©å‰

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**é—®é¢˜ï¼š** Diffusion éœ€è¦å¤šæ¬¡è¿­ä»£é‡‡æ ·ï¼ˆæ…¢ï¼‰

**ç°æœ‰æ–¹æ³•ï¼š**
- **Distillation:** è®­ç»ƒä¸€ä¸ªæ›´å°çš„æ¨¡å‹æ¥æ¨¡æ‹Ÿå¤šæ­¥å»å™ª
- **Fewer Steps:** å‡å°‘é‡‡æ ·æ­¥æ•°ï¼ˆä½†è´¨é‡ä¸‹é™ï¼‰

**æœ¬æ–‡æ–¹æ³•ï¼š**
- **Analytical Approximation:** ç”¨è§£ææ–¹æ³•è¿‘ä¼¼é‡‡æ ·è¿‡ç¨‹
- **Coresets/Clusters:** ç”¨å°‘é‡ä»£è¡¨ç‚¹è¿‘ä¼¼æ•´ä¸ªæ•°æ®åˆ†å¸ƒ

### ğŸ“ æŠ€æœ¯ç»†èŠ‚

**å…³é”®å…¬å¼ï¼š**
```
æ ‡å‡† Diffusion:
  x_{t-1} = Î¼_Î¸(x_t, t) + Ïƒ_t * Îµ

Analytical Approximation:
  x_{t-1} â‰ˆ A_t * x_t + b_t
  
  å…¶ä¸­ A_t, b_t é€šè¿‡ coreset ä¼˜åŒ–å¾—åˆ°
```

### ğŸ’¡ å¯¹ä½ çš„ä»·å€¼

**æ€è·¯å€Ÿé‰´ï¼š**
- å¯ä»¥ç”¨ç±»ä¼¼æ–¹æ³•åŠ é€Ÿ Diffusion LM æ¨ç†
- ç»“åˆå‰ªæï¼šå…ˆå‰ªæï¼Œå† analytical approximation
- å¯èƒ½è¾¾åˆ° 10x+ åŠ é€Ÿ

---

## 3ï¸âƒ£ Hardware-Aware DNN Compression

**ğŸ“„ DBLP:** [abs-2312-15322](https://dblp.org/rec/journals/corr/abs-2312-15322.html)  
**ğŸ“… å‘å¸ƒ:** 4 å¤©å‰

### ğŸ¯ æ ¸å¿ƒæ€æƒ³

**è”åˆä¼˜åŒ–ï¼š**
- **Pruning:** ç»“æ„åŒ–å‰ªæï¼ˆchannel/filter çº§åˆ«ï¼‰
- **Quantization:** Mixed-precisionï¼ˆä¸åŒå±‚ç”¨ä¸åŒç²¾åº¦ï¼‰
- **Hardware-Aware:** è€ƒè™‘å®é™…ç¡¬ä»¶ç‰¹æ€§ï¼ˆå»¶è¿Ÿã€èƒ½è€—ï¼‰

### ğŸ“ æ–¹æ³•

**ä¼˜åŒ–ç›®æ ‡ï¼š**
```
min Accuracy_Loss(W, P, Q)
s.t. Latency(W, P, Q) â‰¤ Target

W: æƒé‡
P: å‰ªæç­–ç•¥
Q: é‡åŒ–é…ç½®
```

### ğŸ’¡ å¯¹ä½ çš„ä»·å€¼

**å¯ä»¥ç›´æ¥ç”¨ï¼š**
- å¦‚æœä½ åœ¨åšè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- éœ€è¦åŒæ—¶è€ƒè™‘å‰ªæ + é‡åŒ–
- å¯ä»¥ç”¨ä»–ä»¬çš„æ¡†æ¶åš hardware-aware ä¼˜åŒ–

---

## ğŸ“ˆ æ€»ç»“ï¼šå¯¹ä½ çš„ç ”ç©¶å»ºè®®

### ä¼˜å…ˆçº§æ’åº

| æ–¹å‘ | ä¼˜å…ˆçº§ | ç†ç”± |
|------|--------|------|
| **Sink-Aware Pruning** | â­â­â­â­â­ | ç›´æ¥é’ˆå¯¹ DLMï¼Œä»£ç å¯ç”¨ |
| **Joint Pruning+Quant** | â­â­â­â­ | ç¡¬ä»¶ååŒï¼Œå®ç”¨æ€§å¼º |
| **Analytical Diffusion** | â­â­â­ | æ€è·¯å€Ÿé‰´ï¼Œéœ€è¦æ”¹ç¼– |

### ä¸‹å‘¨å¯ä»¥åšçš„å®éªŒ

1. **å¤ç° Sink Variance åˆ†æ**ï¼ˆ1-2 å¤©ï¼‰
   - ç”¨ LLaDA æˆ–å…¶ä»– DLM
   - å¯è§†åŒ– sink ä½ç½®å˜åŒ–
   - éªŒè¯è®ºæ–‡ç»“è®º

2. **å®ç° Sink-Aware Pruning**ï¼ˆ3-5 å¤©ï¼‰
   - åŸºäºè®ºæ–‡ä¼ªä»£ç 
   - åœ¨å°å‹æ¨¡å‹ä¸Šæµ‹è¯•
   - å¯¹æ¯” baseline

3. **æ¢ç´¢ Joint Optimization**ï¼ˆ1-2 å‘¨ï¼‰
   - ç»“åˆå‰ªæå’Œé‡åŒ–
   - åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šæµ‹è¯•å»¶è¿Ÿ
   - æ‰¾åˆ°æœ€ä¼˜é…ç½®

---

*æ˜å¤©ç»§ç»­è¿½è¸ªæ–°è®ºæ–‡ â€¢ è¿”å› [00-daily-updates.md](00-daily-updates.md)*
