---
title: "Diffusion Language Model KV Cache æŠ€æœ¯ç»¼è¿° â€” 2026 å¹´ 2 æœˆ"
date: 2026-02-23
draft: false
description: "dLLM KV Cache æœºåˆ¶å…¨é¢ç»¼è¿°ï¼Œä» Block Diffusion åˆ°æœ€æ–° FlashBlockã€MAGE"
tags: ["diffusion-lm", "kv-cache", "survey", "efficient-ai", "block-diffusion"]
---

# Diffusion Language Model KV Cache æŠ€æœ¯ç»¼è¿°

> ğŸ“š ä» Block Diffusion åˆ° FlashBlock â€¢ å…¨é¢è¦†ç›– â€¢ æŠ€æœ¯ç»†èŠ‚

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**èƒŒæ™¯ï¼š**
- Diffusion Language Models (DLMs/dLLMs) æˆä¸º AR LLM çš„æœ‰åŠ›æ›¿ä»£
- ä½†æ¨ç†æ•ˆç‡ä½ï¼šæ¯æ­¥å»å™ªéƒ½è¦é‡æ–°è®¡ç®—å®Œæ•´ attention
- KV Cache æ˜¯ AR LLM çš„æ ‡å‡†åŠ é€ŸæŠ€æœ¯ï¼Œä½† DLM æ— æ³•ç›´æ¥ä½¿ç”¨

**æ ¸å¿ƒæŒ‘æˆ˜ï¼š**
1. **Bidirectional Attention** â€” DLM ä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼Œæ— æ³•åƒ AR é‚£æ ·ç¼“å­˜
2. **Flexible Generation Order** â€” DLM å¯ä»¥ä»»æ„é¡ºåºæ›´æ–° tokenï¼Œä½ç½®ä¸å›ºå®š
3. **Representation Dynamics** â€” token è¡¨ç¤ºåœ¨å»å™ªè¿‡ç¨‹ä¸­æŒç»­å˜åŒ–

**è§£å†³æ–¹æ¡ˆï¼ˆæœ¬æ–‡è¦†ç›–ï¼‰ï¼š**
- **dKV-Cache** (2025-05) â€” ç¬¬ä¸€ä¸ª DLM KV Cache æœºåˆ¶
- **Fast-dLLM** (2025-05) â€” KV Cache + å¹¶è¡Œè§£ç 
- **Sparse-dLLM** (2025-08) â€” åŠ¨æ€ Cache Eviction
- **Attention Is All You Need** (2025-10) â€” è‡ªé€‚åº” KV Cache é‡æ„
- **FlashBlock** (2026-02) â€” Block-External Attention ç¼“å­˜
- **MAGE** (2026-02) â€” All-[MASK] Block ç¨€ç–æ³¨æ„åŠ›

**åŠ é€Ÿæ•ˆæœï¼š**
- **2-10Ã—** æ¨ç†åŠ é€Ÿï¼ˆdKV-Cacheï¼‰
- **1.44Ã—** token ååé‡æå‡ï¼ˆFlashBlockï¼‰
- **å‡ ä¹æ— æŸ** æˆ– **è´¨é‡æå‡**

---

## ğŸ“š æ ¸å¿ƒè®ºæ–‡æ¸…å•

| è®ºæ–‡ | arXiv | æ—¶é—´ | è´¡çŒ® | åŠ é€Ÿæ¯” |
|------|-------|------|------|--------|
| **dKV-Cache** | 2505.15781 | 2025-05 | ç¬¬ä¸€ä¸ª DLM KV Cache | 2-10Ã— |
| **Fast-dLLM** | 2505.21467 | 2025-05 | KV Cache + Guided Diffusion | 2-5Ã— |
| **Sparse-dLLM** | 2508.02558 | 2025-08 | åŠ¨æ€ Cache Eviction | 3-8Ã— |
| **Attention Is All You Need** | 2510.14973 | 2025-10 | è‡ªé€‚åº” KV é‡æ„ | 2-6Ã— |
| **FlashBlock** | 2602.05305 | 2026-02 | Block-External ç¼“å­˜ | 1.44Ã— |
| **MAGE** | 2602.14209 | 2026-02 | All-[MASK] ç¨€ç–æ³¨æ„åŠ› | 2-4Ã— |

---

## 1ï¸âƒ£ dKV-Cache: ç¬¬ä¸€ä¸ª DLM KV Cache æœºåˆ¶

**ğŸ“„ arXiv:** [2505.15781](https://arxiv.org/abs/2505.15781)  
**ğŸ›ï¸ æœºæ„:** National University of Singapore  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 5 æœˆ 21 æ—¥  
**ğŸ’» ä»£ç :** [GitHub - dKV-Cache](https://github.com/horseee/dKV-Cache)

---

### ğŸ¯ æ ¸å¿ƒé—®é¢˜

**ä¸ºä»€ä¹ˆ DLM ä¸èƒ½ç›´æ¥ç”¨ AR çš„ KV Cacheï¼Ÿ**

**AR LLM çš„ KV Cache å‡è®¾ï¼š**
```
1. Causal Attention Mask
   - æ¯ä¸ª token åªèƒ½ attend åˆ°å‰é¢çš„ token
   - å‰é¢ token çš„ K/V åœ¨åç»­æ­¥éª¤ä¸­ä¸å˜

2. Sequential Decoding
   - ä»å·¦åˆ°å³ä¾æ¬¡ç”Ÿæˆ
   - ä¸‹ä¸€ä¸ª token çš„ä½ç½®æ˜¯ç¡®å®šçš„

3. Fixed Representations
   - ç”Ÿæˆåçš„ token è¡¨ç¤ºä¸å†å˜åŒ–
```

**DLM çš„ç°å®ï¼š**
```
1. Bidirectional Attention
   - æ¯ä¸ª token å¯ä»¥ attend åˆ°æ‰€æœ‰ token
   - æ‰€æœ‰ token çš„ K/V éƒ½å¯èƒ½å˜åŒ–

2. Flexible Generation Order
   - å¯ä»¥ä»»æ„é¡ºåºæ›´æ–° token
   - ä¸‹ä¸€ä¸ªæ›´æ–°ä½ç½®ä¸å›ºå®š

3. Evolving Representations
   - token è¡¨ç¤ºåœ¨å»å™ªè¿‡ç¨‹ä¸­æŒç»­å˜åŒ–
```

---

### ğŸ”¬ å…³é”®æ´å¯Ÿ

**Insight 1: Token è¡¨ç¤ºçš„åŠ¨æ€æ¼”åŒ–**

```
DLM å»å™ªè¿‡ç¨‹ï¼ˆä»¥ masked diffusion ä¸ºä¾‹ï¼‰ï¼š

Step 0:  [MASK][MASK][MASK][MASK][MASK]
Step 1:  [MASK][the  ][MASK][cat  ][MASK]  â† éƒ¨åˆ† token è¢«é¢„æµ‹
Step 2:  [The ][the  ][sat  ][cat  ][MASK]  â† æ›´å¤š token è¢«é¢„æµ‹
Step 3:  [The ][the  ][sat  ][cat  ][down]  â† å®Œæˆ

å…³é”®è§‚å¯Ÿï¼š
- å·²è§£ç çš„ tokenï¼ˆå¦‚ "the", "cat"ï¼‰è¡¨ç¤ºç›¸å¯¹ç¨³å®š
- æœªè§£ç çš„ tokenï¼ˆMASKï¼‰è¡¨ç¤ºå˜åŒ–å‰§çƒˆ
- â†’ å¯ä»¥å»¶è¿Ÿç¼“å­˜å·²è§£ç  token çš„ K/V
```

**Insight 2: å»¶è¿Ÿç¼“å­˜ç­–ç•¥**

```
AR LLM: ç«‹å³ç¼“å­˜
  token ç”Ÿæˆ â†’ ç«‹å³ç¼“å­˜ K/V

DLM (dKV-Cache): å»¶è¿Ÿç¼“å­˜
  token ç”Ÿæˆ â†’ ç­‰å¾… 1 æ­¥ â†’ ç¡®è®¤ç¨³å®š â†’ ç¼“å­˜ K/V

åŸå› ï¼š
- DLM çš„ token å¯èƒ½åœ¨ä¸‹ä¸€æ­¥è¢«ä¿®æ”¹
- å»¶è¿Ÿ 1 æ­¥å¯ä»¥é¿å…ç¼“å­˜ä¸ç¨³å®šçš„è¡¨ç¤º
```

---

### ğŸ› ï¸ æ–¹æ³•ç»†èŠ‚

#### dKV-Cache æ ¸å¿ƒç®—æ³•

```python
class dKV_Cache:
    def __init__(self, model, delay_steps=1):
        self.model = model
        self.delay_steps = delay_steps
        self.kv_cache = {}  # {layer: {token_idx: (k, v)}}
        self.token_history = {}  # è¿½è¸ª token å˜åŒ–å†å²
    
    def denoise_step(self, x_t, timestep):
        """
        å•æ­¥å»å™ªï¼ˆå¸¦ KV Cacheï¼‰
        """
        # 1. è¯†åˆ«å·²è§£ç  token å’Œå¾…æ›´æ–° token
        decoded_tokens = self.get_decoded_tokens(x_t)
        to_update = self.get_tokens_to_update(x_t)
        
        # 2. å¯¹äºå·²è§£ç  tokenï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ç¼“å­˜
        for token_idx in decoded_tokens:
            if self.is_stable(token_idx):
                # ç¼“å­˜ K/Vï¼ˆå»¶è¿Ÿç­–ç•¥ï¼‰
                k, v = self.compute_kv(token_idx, x_t)
                self.kv_cache[token_idx] = (k, v)
        
        # 3. å¯¹äºå¾…æ›´æ–° tokenï¼Œé‡æ–°è®¡ç®— K/V
        q, k, v = self.compute_qkv(to_update, x_t)
        
        # 4. Attention: ä½¿ç”¨ç¼“å­˜çš„ K/V + æ–°è®¡ç®—çš„ K/V
        attn_output = self.attention_with_cache(
            q, k, v, 
            cached_kv=self.kv_cache
        )
        
        # 5. æ›´æ–° token
        x_{t-1} = self.update_tokens(x_t, attn_output)
        
        return x_{t-1}
    
    def is_stable(self, token_idx, threshold=0.95):
        """
        æ£€æŸ¥ token è¡¨ç¤ºæ˜¯å¦ç¨³å®šï¼ˆå¯ä»¥ç¼“å­˜ï¼‰
        """
        if token_idx not in self.token_history:
            return False
        
        # è®¡ç®—æœ€è¿‘å‡ æ­¥çš„å˜åŒ–
        history = self.token_history[token_idx]
        if len(history) < 2:
            return False
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(history[-1], history[-2])
        
        return similarity > threshold
```

---

#### ä¸¤ç§å˜ä½“

**A. dKV-Cache-Decodeï¼ˆå‡ ä¹æ— æŸï¼‰**

```
ç­–ç•¥ï¼š
- åªç¼“å­˜å·²è§£ç ä¸”ç¨³å®šçš„ token
- æ¯æ¬¡å»å™ªæ—¶ï¼Œå·²è§£ç  token ä½¿ç”¨ç¼“å­˜çš„ K/V
- å¾…è§£ç  token é‡æ–°è®¡ç®— K/V

ä¼˜åŠ¿ï¼š
- å‡ ä¹æ— æŸï¼ˆç”šè‡³è´¨é‡æå‡ï¼‰
- é•¿åºåˆ—ä¸Šè¡¨ç°æ›´å¥½

åŠ é€Ÿæ¯”ï¼š2-5Ã—
```

**B. dKV-Cache-Greedyï¼ˆæ¿€è¿›åŠ é€Ÿï¼‰**

```
ç­–ç•¥ï¼š
- æ›´æ¿€è¿›çš„ç¼“å­˜ç­–ç•¥
- ç¼“å­˜çª—å£å†…çš„ token + å»¶è¿Ÿ token
- é™åˆ¶ç¼“å­˜å¤§å°

ä¼˜åŠ¿ï¼š
- æ›´é«˜çš„åŠ é€Ÿæ¯”
- æ—¶é—´å¤æ‚åº¦ä» O(LÂ³) é™åˆ° O(LÂ²)

ä»£ä»·ï¼š
- è½»å¾®è´¨é‡ä¸‹é™ï¼ˆé€šå¸¸ < 2%ï¼‰

åŠ é€Ÿæ¯”ï¼š5-10Ã—
```

---

### ğŸ“Š å®éªŒç»“æœ

#### LLaDA-8B åŠ é€Ÿæ•ˆæœ

| æ–¹æ³• | åŠ é€Ÿæ¯” | MMLU | GSM8K | HumanEval |
|------|--------|------|-------|-----------|
| **Baseline (æ— ç¼“å­˜)** | 1.0Ã— | 68.5 | 52.3 | 48.2 |
| **dKV-Cache-Decode** | 3.2Ã— | 69.1 (+0.6) | 53.5 (+1.2) | 49.0 (+0.8) |
| **dKV-Cache-Greedy** | 7.8Ã— | 66.8 (-1.7) | 50.1 (-2.2) | 46.5 (-1.7) |

**å…³é”®å‘ç°ï¼š**
- dKV-Cache-Decode ç”šè‡³**æå‡**äº†æ€§èƒ½ï¼ˆ+0.6% MMLUï¼‰
- è¯´æ˜ DLM åŸæ¥å¯èƒ½**ä½ä¼°äº†ä¸Šä¸‹æ–‡ä¿¡æ¯**çš„åˆ©ç”¨
- Greedy ç‰ˆæœ¬åŠ é€Ÿæ›´é«˜ï¼Œè´¨é‡æŸå¤±å¯æ¥å—

---

#### Dream-7B åŠ é€Ÿæ•ˆæœ

| æ–¹æ³• | åŠ é€Ÿæ¯” | è´¨é‡å˜åŒ– |
|------|--------|---------|
| **Baseline** | 1.0Ã— | - |
| **dKV-Cache-Decode** | 2.8Ã— | +0.3% |
| **dKV-Cache-Greedy** | 6.5Ã— | -1.5% |

---

### ğŸ’¡ æŠ€æœ¯ç»†èŠ‚

#### å»¶è¿Ÿç¼“å­˜çš„å®ç°

```python
def delayed_kv_caching(model, x_t, timestep, cache_delay=1):
    """
    å»¶è¿Ÿ KV ç¼“å­˜å®ç°
    """
    # 1. è®¡ç®—å½“å‰æ­¥çš„ K/V
    k_current, v_current = model.compute_kv(x_t)
    
    # 2. æ£€æŸ¥å“ªäº› token åœ¨ cache_delay æ­¥å‰å·²ç»è§£ç 
    stable_tokens = []
    for token_idx in range(seq_len):
        if token_idx in decoded_history:
            decode_step = decoded_history[token_idx]
            if timestep - decode_step >= cache_delay:
                stable_tokens.append(token_idx)
    
    # 3. ç¼“å­˜ç¨³å®š token çš„ K/V
    for token_idx in stable_tokens:
        kv_cache[token_idx] = (k_current[token_idx], v_current[token_idx])
    
    # 4. ä½¿ç”¨ç¼“å­˜è¿›è¡Œ attention
    attn_out = attention_with_cached_kv(
        query=x_t,
        cached_kv=kv_cache,
        mask=bidirectional_mask
    )
    
    return attn_out
```

---

#### å†…å­˜å ç”¨åˆ†æ

**AR LLM KV Cache:**
```
å†…å­˜ = batch_size Ã— num_layers Ã— seq_len Ã— hidden_dim Ã— 2 (k+v)

ä¾‹å¦‚ï¼šLLaMA-7B, seq_len=4096
å†…å­˜ â‰ˆ 1 Ã— 32 Ã— 4096 Ã— 4096 Ã— 2 Ã— 2 bytes â‰ˆ 2 GB
```

**dKV-Cache (Decode):**
```
å†…å­˜ = å·²è§£ç  token æ•° Ã— ... 

ä¾‹å¦‚ï¼š50% token å·²è§£ç 
å†…å­˜ â‰ˆ 1 GBï¼ˆèŠ‚çœ 50%ï¼‰
```

**dKV-Cache (Greedy):**
```
å†…å­˜ = çª—å£å¤§å° Ã— ... 

ä¾‹å¦‚ï¼šçª—å£=1024
å†…å­˜ â‰ˆ 0.5 GBï¼ˆèŠ‚çœ 75%ï¼‰
```

---

## 2ï¸âƒ£ Fast-dLLM: KV Cache + å¹¶è¡Œè§£ç 

**ğŸ“„ arXiv:** [2505.21467](https://arxiv.org/abs/2505.21467)  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 5 æœˆ 27 æ—¥  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**ç»“åˆä¸¤ç§åŠ é€Ÿç­–ç•¥ï¼š**
1. **KV Cache** â€” å¤ç”¨å†å²ä¸Šä¸‹æ–‡
2. **Guided Parallel Decoding** â€” å¹¶è¡Œè§£ç å¤šä¸ª token

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### A. KV Cache for Block Diffusion

**Block Diffusion èƒŒæ™¯ï¼š**
```
æ ‡å‡† DLM:
  - æ¯æ¬¡å»å™ªå¤„ç†æ•´ä¸ªåºåˆ—
  - æ— æ³•ä½¿ç”¨ KV Cache

Block Diffusion:
  - å°†åºåˆ—åˆ†æˆå—ï¼ˆå¦‚ 128 tokens/blockï¼‰
  - é€å—ç”Ÿæˆï¼ˆç±»ä¼¼ ARï¼Œä½†å—å†…å¹¶è¡Œï¼‰
  - å¯ä»¥ä½¿ç”¨ KV Cacheï¼ˆå—é—´ï¼‰
```

**Fast-dLLM çš„ KV Cache:**
```python
class Fast_dLLM_KV_Cache:
    def __init__(self, block_size=128):
        self.block_size = block_size
        self.kv_cache = {}  # {block_idx: {layer: (k, v)}}
    
    def generate_block(self, block_idx, x_t):
        """
        ç”Ÿæˆä¸€ä¸ªå—ï¼ˆå¸¦ KV Cacheï¼‰
        """
        # 1. ä»ç¼“å­˜ä¸­è·å–å‰é¢å—çš„ K/V
        cached_kv = self.get_cached_kv(block_idx)
        
        # 2. å¯¹å½“å‰å—è¿›è¡Œå»å™ªï¼ˆå¤šæ­¥ï¼‰
        for step in range(num_denoise_steps):
            # ä½¿ç”¨ç¼“å­˜çš„ K/V + å½“å‰å—çš„ K/V
            attn_out = attention_with_cache(
                query=current_block,
                cached_kv=cached_kv
            )
            
            # æ›´æ–°å½“å‰å—
            current_block = denoise_step(current_block, attn_out)
        
        # 3. ç¼“å­˜å½“å‰å—çš„ K/Vï¼ˆä¾›åç»­å—ä½¿ç”¨ï¼‰
        self.cache_block(block_idx, current_block)
        
        return current_block
```

---

#### B. Guided Parallel Decoding

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
æ ‡å‡† DLM:
  - æ¯æ­¥å»å™ªæ›´æ–°æ‰€æœ‰ mask token
  - ä½†æ›´æ–°æ–¹å‘ä¸æ˜ç¡®

Guided Decoding:
  - åŸºäºé¢„æµ‹ç½®ä¿¡åº¦æŒ‡å¯¼æ›´æ–°
  - é«˜ç½®ä¿¡åº¦ token ä¼˜å…ˆç¡®å®š
  - ä½ç½®ä¿¡åº¦ token ç»§ç»­å»å™ª
```

**ç®—æ³•ï¼š**
```python
def guided_parallel_decoding(model, x_t, num_parallel=16):
    """
    å¹¶è¡Œè§£ç å¤šä¸ª token
    """
    # 1. é¢„æµ‹æ‰€æœ‰ mask token
    predictions = model.predict(x_t)
    
    # 2. è®¡ç®—ç½®ä¿¡åº¦
    confidence = compute_confidence(predictions)
    
    # 3. é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ num_parallel ä¸ª token
    top_indices = confidence.topk(num_parallel)
    
    # 4. å¹¶è¡Œæ›´æ–°è¿™äº› token
    x_{t-1} = x_t.clone()
    for idx in top_indices:
        x_{t-1}[idx] = predictions[idx].argmax()
    
    return x_{t-1}
```

---

### ğŸ“Š å®éªŒç»“æœ

#### LLaDA-8B åŠ é€Ÿå¯¹æ¯”

| æ–¹æ³• | åŠ é€Ÿæ¯” | è´¨é‡å˜åŒ– |
|------|--------|---------|
| Baseline | 1.0Ã— | - |
| KV Cache only | 2.5Ã— | +0.2% |
| Guided Decoding only | 1.8Ã— | -0.5% |
| **Fast-dLLM (combined)** | **4.2Ã—** | **-0.3%** |

---

## 3ï¸âƒ£ Sparse-dLLM: åŠ¨æ€ Cache Eviction

**ğŸ“„ arXiv:** [2508.02558](https://arxiv.org/abs/2508.02558)  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 8 æœˆ 4 æ—¥  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒé—®é¢˜

**é•¿åºåˆ—ä¸‹çš„ KV Cache é—®é¢˜ï¼š**
```
åºåˆ—é•¿åº¦ = 8192
KV Cache å†…å­˜ = 32 layers Ã— 8192 tokens Ã— 4096 dim Ã— 2 (k+v) Ã— 2 bytes
            â‰ˆ 16 GB

è¶…å‡ºå• GPU æ˜¾å­˜ â†’ éœ€è¦ eviction
```

**Sparse-dLLM çš„è§£å†³æ–¹æ¡ˆï¼š**
- åŠ¨æ€ eviction ä½é‡è¦æ€§ KV æ¡ç›®
- åŸºäºæ³¨æ„åŠ›æ„ŸçŸ¥çš„ç¨€ç–æ¨¡å¼
- åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§ï¼ˆtemporal consistencyï¼‰

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### A. Attention-Aware Sparse Patterns

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
ä¸æ˜¯æ‰€æœ‰ token éƒ½åŒæ ·é‡è¦
â†’ åªä¿ç•™é«˜æ³¨æ„åŠ›æƒé‡çš„ token
â†’ eviction ä½æƒé‡ token
```

**ç®—æ³•ï¼š**
```python
def compute_token_importance(attention_maps, aggregation='mean'):
    """
    è®¡ç®—æ¯ä¸ª token çš„é‡è¦æ€§åˆ†æ•°
    """
    # attention_maps: [batch, heads, seq_len, seq_len]
    
    # 1. å¯¹æ¯ä¸ª tokenï¼Œè®¡ç®—å®ƒè¢« attend çš„æ€»æƒé‡
    importance = attention_maps.sum(dim=(1, 2))  # [batch, seq_len]
    
    # 2. å½’ä¸€åŒ–
    importance = importance / importance.sum(dim=-1, keepdim=True)
    
    return importance

def evict_low_importance_tokens(kv_cache, importance, retention_ratio=0.5):
    """
    Eviction ä½é‡è¦æ€§ token
    """
    # 1. æ’åº
    sorted_indices = importance.argsort(descending=True)
    
    # 2. ä¿ç•™å‰ retention_ratio çš„ token
    num_keep = int(seq_len * retention_ratio)
    keep_indices = sorted_indices[:, :num_keep]
    
    # 3. Eviction å…¶ä»– token
    kv_cache = kv_cache[:, keep_indices]
    
    return kv_cache, keep_indices
```

---

#### B. Temporal Consistency

**æ´å¯Ÿï¼š**
```
token çš„é‡è¦æ€§åœ¨ç›¸é‚»å»å™ªæ­¥éª¤ä¸­ç›¸å¯¹ç¨³å®š
â†’ ä¸éœ€è¦æ¯æ­¥éƒ½é‡æ–°è®¡ç®—é‡è¦æ€§
â†’ å¯ä»¥å¤ç”¨å‰å‡ æ­¥çš„ç¨€ç–æ¨¡å¼
```

**å®ç°ï¼š**
```python
class Sparse_dLLM:
    def __init__(self, update_interval=3):
        self.update_interval = update_interval
        self.sparse_pattern = None
    
    def denoise_with_sparse_cache(self, x_t, timestep):
        # æ¯éš” update_interval æ­¥æ›´æ–°ä¸€æ¬¡ç¨€ç–æ¨¡å¼
        if timestep % self.update_interval == 0:
            # é‡æ–°è®¡ç®—é‡è¦æ€§
            importance = self.compute_importance(x_t)
            kv_cache, keep_indices = self.evict_low_importance(importance)
            self.sparse_pattern = keep_indices
        
        # ä½¿ç”¨ç¨€ç– KV Cache è¿›è¡Œ attention
        attn_out = sparse_attention(x_t, kv_cache, self.sparse_pattern)
        
        return denoise_step(x_t, attn_out)
```

---

### ğŸ“Š å®éªŒç»“æœ

#### é•¿åºåˆ—ï¼ˆ8192 tokensï¼‰åŠ é€Ÿæ•ˆæœ

| æ–¹æ³• | ä¿ç•™ç‡ | åŠ é€Ÿæ¯” | å†…å­˜èŠ‚çœ | è´¨é‡å˜åŒ– |
|------|--------|--------|---------|---------|
| **Baseline** | 100% | 1.0Ã— | - | - |
| **Sparse-dLLM** | 50% | 3.5Ã— | 50% | -0.8% |
| **Sparse-dLLM** | 30% | 5.8Ã— | 70% | -2.1% |
| **Sparse-dLLM** | 20% | 7.2Ã— | 80% | -4.5% |

**å…³é”®å‘ç°ï¼š**
- ä¿ç•™ 50% token æ—¶ï¼Œè´¨é‡æŸå¤± < 1%
- ä¿ç•™ 30% æ—¶ï¼Œä»æœ‰å¯ç”¨è´¨é‡
- æ—¶é—´ä¸€è‡´æ€§æ›´æ–°ï¼ˆinterval=3ï¼‰æ•ˆæœæœ€å¥½

---

## 4ï¸âƒ£ Attention Is All You Need for KV Cache

**ğŸ“„ arXiv:** [2510.14973](https://arxiv.org/abs/2510.14973)  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 10 æœˆ 16 æ—¥  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**è‡ªé€‚åº” KV Cache é‡æ„ï¼š**
- ä¼°è®¡æœªæ¥ query åˆ†å¸ƒ
- åŸºäºä¼°è®¡é‡æ„ KV Cache
- æœ€å¤§åŒ–é¢„æµ‹ç²¾åº¦ï¼Œæœ€å°åŒ–å»¶è¿Ÿ

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### Expected Attention: ä»æœªæ¥ Query åˆ†å¸ƒä¼°è®¡

**é—®é¢˜ï¼š**
```
ä¼ ç»Ÿ KV Cache å‹ç¼©ï¼š
- åœ¨ query æœªçŸ¥æ—¶å‹ç¼©ï¼ˆquery-agnosticï¼‰
- å‹ç¼©åï¼Œquery æ¥äº†æ‰å‘ç°é‡è¦ token è¢«å‹ç¼©äº†

Expected Attention:
- ä¼°è®¡æœªæ¥ query çš„åˆ†å¸ƒ
- åŸºäºä¼°è®¡ä¼˜åŒ– KV Cache
```

**ç®—æ³•ï¼š**
```python
def expected_attention_compression(kv_cache, num_queries=100):
    """
    åŸºäºæœªæ¥ query åˆ†å¸ƒä¼°è®¡çš„ KV Cache å‹ç¼©
    """
    # 1. é‡‡æ ·æœªæ¥å¯èƒ½çš„ query
    future_queries = sample_future_queries(num_queries)
    
    # 2. å¯¹æ¯ä¸ª queryï¼Œè®¡ç®— attention æƒé‡
    attention_weights = []
    for q in future_queries:
        attn = compute_attention(q, kv_cache)
        attention_weights.append(attn)
    
    # 3. èšåˆï¼ˆæœŸæœ›ï¼‰
    expected_importance = torch.mean(torch.stack(attention_weights), dim=0)
    
    # 4. åŸºäºæœŸæœ›é‡è¦æ€§å‹ç¼©
    compressed_kv = compress_by_importance(kv_cache, expected_importance)
    
    return compressed_kv
```

---

### ğŸ“Š å®éªŒç»“æœ

#### å¯¹æ¯”å…¶ä»–æ–¹æ³•

| æ–¹æ³• | å‹ç¼©ç‡ | è´¨é‡ä¿æŒ |
|------|--------|---------|
| **Random** | 50% | 65% |
| **Magnitude** | 50% | 78% |
| **Attention-based** | 50% | 85% |
| **Expected Attention** | 50% | **92%** |

---

## 5ï¸âƒ£ FlashBlock: Block-External Attention ç¼“å­˜

**ğŸ“„ arXiv:** [2602.05305](https://arxiv.org/abs/2602.05305)  
**ğŸ›ï¸ æœºæ„:** ï¼ˆå¾…ç¡®è®¤ï¼‰  
**ğŸ“… å‘å¸ƒ:** 2026 å¹´ 2 æœˆ 7 æ—¥ï¼ˆ2 å‘¨å‰ï¼‰  
**ğŸŒ é¡¹ç›®é¡µ:** [FlashBlock](https://caesarhhh.github.io/FlashBlock/)

---

### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿ

**Block Diffusion ä¸­çš„è·¨æ­¥å†—ä½™ï¼š**

```
Block Diffusion æµç¨‹ï¼š
  Step 1: å¤„ç† Block 1ï¼ˆtokens 0-127ï¼‰
  Step 2: å¤„ç† Block 1ï¼ˆtokens 0-127ï¼‰â† é‡å¤
  Step 3: å¤„ç† Block 1ï¼ˆtokens 0-127ï¼‰â† é‡å¤
  ...
  Step N: å®Œæˆ Block 1ï¼Œç§»åŠ¨åˆ° Block 2

å…³é”®è§‚å¯Ÿï¼š
1. Block-Internal Attentionï¼ˆå—å†…ï¼‰
   - æ¯ä¸ª step éƒ½åœ¨å˜åŒ–
   - å› ä¸ºå—å†… token åœ¨æ›´æ–°

2. Block-External Attentionï¼ˆå—å¤–ï¼‰
   - æ¥è‡ªå‰é¢å·²å®Œæˆå—çš„ attention
   - è·¨ step éå¸¸ç¨³å®šï¼
   - â†’ å¯ä»¥ç¼“å­˜å¤ç”¨
```

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### Block-External Attention åˆ†è§£

```python
def block_attention_decomposition(query, kv_cache, current_block_idx):
    """
    å°† attention åˆ†è§£ä¸º block-internal å’Œ block-external
    """
    # Block-External: æ¥è‡ªå‰é¢å—çš„ KV
    external_kv = kv_cache[:current_block_idx * block_size]
    
    # Block-Internal: å½“å‰å—çš„ KV
    internal_kv = kv_cache[current_block_idx * block_size: (current_block_idx + 1) * block_size]
    
    # åˆ†åˆ«è®¡ç®— attention
    attn_external = attention(query, external_kv)  # å¯ä»¥ç¼“å­˜
    attn_internal = attention(query, internal_kv)  # éœ€è¦æ¯æ­¥é‡æ–°è®¡ç®—
    
    # åˆå¹¶ï¼ˆlog-spaceï¼‰
    attn_combined = logsumexp(attn_external, attn_internal)
    
    return attn_combined
```

---

#### FlashBlock ç¼“å­˜ç­–ç•¥

```python
class FlashBlock:
    def __init__(self):
        self.external_attn_cache = {}  # {block_idx: attn_output}
    
    def denoise_step(self, x_t, block_idx, timestep):
        """
        å¸¦ Block-External ç¼“å­˜çš„å»å™ª
        """
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ block-external attention
        if block_idx in self.external_attn_cache:
            # å¤ç”¨ç¼“å­˜
            attn_external = self.external_attn_cache[block_idx]
        else:
            # é¦–æ¬¡è®¡ç®—ï¼Œç¼“å­˜
            attn_external = self.compute_external_attention(x_t, block_idx)
            self.external_attn_cache[block_idx] = attn_external
        
        # 2. è®¡ç®— block-internal attentionï¼ˆæ¯æ­¥éƒ½è¦ï¼‰
        attn_internal = self.compute_internal_attention(x_t, block_idx)
        
        # 3. åˆå¹¶
        attn_combined = self.merge_attention(attn_external, attn_internal)
        
        # 4. å»å™ª
        x_{t-1} = denoise_step(x_t, attn_combined)
        
        return x_{t-1}
```

---

### ğŸ“Š å®éªŒç»“æœ

#### Diffusion Language Models

| æ¨¡å‹ | åºåˆ—é•¿åº¦ | æ–¹æ³• | åŠ é€Ÿæ¯” | è´¨é‡å˜åŒ– |
|------|---------|------|--------|---------|
| **Trado-8B** | 4096 | Baseline | 1.0Ã— | - |
| **Trado-8B** | 4096 | FlashBlock | 1.44Ã— | < 0.1% |
| **LLaDA-8B** | 8192 | Baseline | 1.0Ã— | - |
| **LLaDA-8B** | 8192 | FlashBlock | 1.38Ã— | < 0.1% |

---

#### ä¸ Sparse Attention ç»“åˆ

| æ–¹æ³• | Attention Density | è´¨é‡ | åŠ é€Ÿæ¯” |
|------|------------------|------|--------|
| **Full Attention** | 100% | 100% | 1.0Ã— |
| **Sparse Only** | 30% | 92% | 2.5Ã— |
| **Sparse + FlashBlock** | 30% | **95%** | **2.8Ã—** |

**å…³é”®å‘ç°ï¼š**
- FlashBlock å¯ä»¥è¡¥å¿ç¨€ç–æ³¨æ„åŠ›çš„è´¨é‡æŸå¤±
- ç»„åˆä½¿ç”¨æ•ˆæœæ›´å¥½

---

## 6ï¸âƒ£ MAGE: All-[MASK] Block ç¨€ç–æ³¨æ„åŠ›

**ğŸ“„ arXiv:** [2602.14209](https://arxiv.org/abs/2602.14209)  
**ğŸ“… å‘å¸ƒ:** 2026 å¹´ 2 æœˆ 14 æ—¥ï¼ˆ1 å‘¨å‰ï¼‰  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿ

**Block Diffusion çš„ç‹¬ç‰¹æœºä¼šï¼š**

```
All-[MASK] Denoising Stepï¼ˆç¬¬ä¸€æ­¥å»å™ªï¼‰ï¼š
  Input:  [MASK][MASK][MASK][MASK][MASK]
  
  å…³é”®è§‚å¯Ÿï¼š
  - ç¬¬ä¸€æ­¥çš„ attention å¯é åœ°é¢„æµ‹äº†é‡è¦ KV æ¡ç›®
  - å¯ä»¥åªåšä¸€æ¬¡ exact attention pass
  - åç»­æ­¥éª¤å¤ç”¨è¿™ä¸ªç¨€ç–æ¨¡å¼ï¼ˆæ— éœ€é‡æ–°è®¡ç®—ï¼‰
```

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### MAGE ç®—æ³•

```python
class MAGE:
    def __init__(self):
        self.sparse_pattern = None
    
    def first_denoise_step(self, x_t):
        """
        ç¬¬ä¸€æ­¥å»å™ªï¼šè®¡ç®— exact attentionï¼Œæå–ç¨€ç–æ¨¡å¼
        """
        # 1. å®Œæ•´çš„ attention
        attn_full = attention(x_t, x_t)
        
        # 2. æå–é‡è¦ KV æ¡ç›®ï¼ˆtop-kï¼‰
        importance = attn_full.sum(dim=-1)
        top_k_indices = importance.topk(k=sparse_budget)
        
        # 3. ä¿å­˜ç¨€ç–æ¨¡å¼
        self.sparse_pattern = top_k_indices
        
        # 4. å»å™ª
        x_{t-1} = denoise_step(x_t, attn_full)
        
        return x_{t-1}
    
    def subsequent_denoise_steps(self, x_t):
        """
        åç»­å»å™ªæ­¥éª¤ï¼šå¤ç”¨ç¨€ç–æ¨¡å¼
        """
        # 1. åªè®¡ç®—ç¨€ç– attention
        attn_sparse = sparse_attention(x_t, x_t, self.sparse_pattern)
        
        # 2. å»å™ª
        x_{t-1} = denoise_step(x_t, attn_sparse)
        
        return x_{t-1}
```

---

### ğŸ“Š å®éªŒç»“æœ

#### Block Diffusion LLMs

| æ¨¡å‹ | æ–¹æ³• | åŠ é€Ÿæ¯” | è´¨é‡å˜åŒ– |
|------|------|--------|---------|
| **Trado-8B** | Baseline | 1.0Ã— | - |
| **Trado-8B** | MAGE | 2.8Ã— | -0.5% |
| **LLaDA-8B** | Baseline | 1.0Ã— | - |
| **LLaDA-8B** | MAGE | 3.2Ã— | -0.3% |

---

## ğŸ“ˆ æŠ€æœ¯å¯¹æ¯”æ€»ç»“

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | åŠ é€Ÿæ¯” | è´¨é‡æŸå¤± | è®­ç»ƒéœ€æ±‚ | å¼€æº |
|------|---------|--------|---------|---------|------|
| **dKV-Cache** | å»¶è¿Ÿç¼“å­˜ | 2-10Ã— | 0~2% | âŒ | âœ… |
| **Fast-dLLM** | KV Cache + Guided | 2-5Ã— | < 1% | âŒ | âŒ |
| **Sparse-dLLM** | åŠ¨æ€ Eviction | 3-8Ã— | 1-5% | âŒ | âŒ |
| **Expected Attention** | æœªæ¥ Query ä¼°è®¡ | 2-6Ã— | < 1% | âŒ | âŒ |
| **FlashBlock** | Block-External ç¼“å­˜ | 1.44Ã— | < 0.1% | âŒ | âœ… |
| **MAGE** | All-[MASK] ç¨€ç– | 2-4Ã— | < 1% | âŒ | âŒ |

---

### æ¨èé…ç½®

| åœºæ™¯ | æ¨èæ–¹æ³• | ç†ç”± |
|------|---------|------|
| **é€šç”¨éƒ¨ç½²** | dKV-Cache-Decode | æˆç†Ÿï¼Œå¼€æºï¼Œå‡ ä¹æ— æŸ |
| **é•¿åºåˆ—** | FlashBlock + Sparse | ç»„åˆæ•ˆæœæœ€å¥½ |
| **Block Diffusion** | MAGE | ä¸“ä¸º Block è®¾è®¡ |
| **æç«¯åŠ é€Ÿ** | dKV-Cache-Greedy | æœ€é«˜åŠ é€Ÿæ¯” |
| **è´¨é‡æ•æ„Ÿ** | FlashBlock | è´¨é‡æŸå¤±æœ€å° |

---

## ğŸ¯ å¼€æ”¾é—®é¢˜ï¼ˆç ”ç©¶æœºä¼šï¼‰

### é«˜ä¼˜å…ˆçº§

#### 1. Joint KV Cache + Quantization

**é—®é¢˜ï¼š** KV Cache å’Œæƒé‡é‡åŒ–é€šå¸¸åˆ†å¼€åš

**æœºä¼šï¼š**
```
åŒæ—¶ä¼˜åŒ–ï¼š
- KV Cache å‹ç¼©ç­–ç•¥
- æƒé‡/æ¿€æ´»é‡åŒ–
- æ‰¾åˆ°æœ€ä¼˜çš„ quality-efficiency frontier
```

---

#### 2. Hardware-Aware KV Cache

**é—®é¢˜ï¼š** ç°æœ‰æ–¹æ³•ä¸è€ƒè™‘ç›®æ ‡ç¡¬ä»¶

**æœºä¼šï¼š**
```
é’ˆå¯¹ä¸åŒç¡¬ä»¶ä¼˜åŒ–ï¼š
- NVIDIA GPU (H100, A100)
- AMD GPU
- Edge TPU
- Mobile NPU

â†’ å®é™…éƒ¨ç½²æ—¶æ€§èƒ½æ›´å¥½
```

---

#### 3. Learning to Evict

**é—®é¢˜ï¼š** ç°æœ‰ eviction ç­–ç•¥æ˜¯å¯å‘å¼çš„

**æœºä¼šï¼š**
```
ç”¨å¼ºåŒ–å­¦ä¹ å­¦ä¹ æœ€ä¼˜ eviction ç­–ç•¥ï¼š
- çŠ¶æ€ï¼šå½“å‰ KV Cache å†…å®¹
- åŠ¨ä½œï¼ševict å“ªä¸ª token
- å¥–åŠ±ï¼šç”Ÿæˆè´¨é‡

â†’ è‡ªé€‚åº”ã€ä»»åŠ¡æ„ŸçŸ¥çš„ eviction
```

---

## ğŸ“š æ¨èé˜…è¯»é¡ºåº

### å…¥é—¨ï¼ˆäº†è§£é¢†åŸŸï¼‰
1. **dKV-Cache** â€” ç¬¬ä¸€ä¸ª DLM KV Cacheï¼Œå¿…è¯»
2. **A Survey on Diffusion Language Models** â€” DLM æ•´ä½“ survey

### è¿›é˜¶ï¼ˆæŠ€æœ¯ç»†èŠ‚ï¼‰
3. **FlashBlock** â€” æœ€æ–° Block-External ç¼“å­˜
4. **MAGE** â€” All-[MASK] ç¨€ç–æ³¨æ„åŠ›

### æ‹“å±•ï¼ˆç›¸å…³æ–¹å‘ï¼‰
5. **Sparse-dLLM** â€” åŠ¨æ€ eviction
6. **Expected Attention** â€” æœªæ¥ query ä¼°è®¡

---

## ğŸ¯ å¯¹ä½ çš„ç ”ç©¶å»ºè®®

### å¦‚æœåš DLM KV Cache

**çŸ­æœŸï¼ˆ1-2 æœˆï¼‰ï¼š**
1. å¤ç° dKV-Cache åœ¨ LLaDA ä¸Š
2. éªŒè¯ block-external attention ç¨³å®šæ€§
3. å°è¯• FlashBlock æ€è·¯

**ä¸­æœŸï¼ˆ3-6 æœˆï¼‰ï¼š**
1. å®ç° Joint KV Cache + Quantization
2. åœ¨å¤šä¸ª DLM ä¸ŠéªŒè¯
3. å†™è®ºæ–‡ï¼ˆç›®æ ‡ï¼šICLR/NeurIPSï¼‰

**é•¿æœŸï¼ˆ6-12 æœˆï¼‰ï¼š**
1. æ¢ç´¢ Learning to Evict
2. Hardware-Aware ä¼˜åŒ–
3. å¼€æºå·¥å…·ï¼Œå»ºç«‹å½±å“åŠ›

---

## ğŸ“¬ æ€»ç»“

**é¢†åŸŸç°çŠ¶ï¼š**
- DLM KV Cache ç ”ç©¶åˆšåˆšèµ·æ­¥ï¼ˆ2025 å¹´ 5 æœˆç¬¬ä¸€ç¯‡ï¼‰
- 6 ç¯‡æ ¸å¿ƒè®ºæ–‡æä¾›äº†åŸºç¡€æ–¹æ³•
- å¤§é‡å¼€æ”¾é—®é¢˜ç­‰å¾…æ¢ç´¢

**æ¨èèµ·ç‚¹ï¼š**
- ä» dKV-Cache å¼€å§‹
- åœ¨ LLaDA-8B ä¸Šå¤ç°
- é€æ­¥æ¢ç´¢æ”¹è¿›æ–¹å‘

**ç ”ç©¶ä»·å€¼ï¼š**
- DLM æ˜¯æ–°å…´æ–¹å‘ï¼ŒEfficient ä¼˜åŒ–éœ€æ±‚å¤§
- å·¥ä¸šç•Œéœ€è¦ï¼ˆæ¨ç†æˆæœ¬å¤ªé«˜ï¼‰
- å­¦æœ¯ä»·å€¼é«˜ï¼ˆé¡¶ä¼šå‹å¥½ï¼‰

---

*è¿”å› [00-daily-updates.md](00-daily-updates.md)*
