---
title: "NanoChat æ·±åº¦è°ƒç ”æŠ¥å‘Š â€” Andrej Karpathy çš„ LLM å®éªŒæ¡†æ¶"
date: 2026-02-23
draft: false
description: "æºç çº§åˆ†æ karpathy/nanochat é¡¹ç›®æ¶æ„ã€è®¾è®¡æ€æƒ³å’Œå¯¹ Efficient AI ç ”ç©¶çš„å¯å‘"
tags: ["nanochat", "karpathy", "llm-training", "research-tools", "deep-dive"]
---

# NanoChat æ·±åº¦è°ƒç ”æŠ¥å‘Š

> ğŸ”¬ æºç çº§åˆ†æ â€¢ è®¾è®¡æ€æƒ³ â€¢ å¯¹ä½ çš„ç ”ç©¶ä»·å€¼

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**NanoChat** æ˜¯ Andrej Karpathy äº 2025 å¹´ 10 æœˆå‘å¸ƒçš„å¼€æºé¡¹ç›®ï¼š

> "ç”¨æœ€ç®€å•çš„ä»£ç ï¼Œåœ¨å• GPU èŠ‚ç‚¹ä¸Šï¼ŒèŠ±~100 ç¾å…ƒè®­ç»ƒä¸€ä¸ª ChatGPT clone"

**æ ¸å¿ƒä»·å€¼ï¼š**
- ğŸ¯ **å…¨æ ˆ pipeline** â€” tokenizerã€é¢„è®­ç»ƒã€å¾®è°ƒã€æ¨ç†ã€Web UI å…¨åŒ…
- ğŸ’° **æä½æˆæœ¬** â€” GPT-2 çº§åˆ«æ¨¡å‹åªéœ€~$72ï¼ˆ3 å°æ—¶ 8Ã—H100ï¼‰
- ğŸ“¦ **æœ€å°ä¾èµ–** â€” çº¯ PyTorchï¼Œä»£ç å¯ hack
- âš¡ **å¿«é€Ÿè¿­ä»£** â€” æ”¯æŒ"speedrun"æ¨¡å¼ï¼Œ3 å°æ—¶å‡ºæ¨¡å‹

**å¯¹ä½ çš„ç ”ç©¶ä»·å€¼ï¼š**
- å­¦ä¹  LLM è®­ç»ƒå…¨æ ˆæµç¨‹
- ç†è§£ Efficient AI å®è·µ
- å¿«é€ŸéªŒè¯æƒ³æ³•çš„å®éªŒå¹³å°
- å€Ÿé‰´è®¾è®¡æ€æƒ³åˆ°è‡ªå·±çš„ç ”ç©¶

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„æ·±åº¦åˆ†æ

### æ•´ä½“è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NanoChat Pipeline                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. Tokenizer Training                                        â”‚
â”‚     â””â”€â†’ è®­ç»ƒè‡ªå®šä¹‰ BPE tokenizer                              â”‚
â”‚     â””â”€â†’ vocab_size=50,257ï¼ˆä¸ GPT-2 å…¼å®¹ï¼‰                    â”‚
â”‚                                                               â”‚
â”‚  2. Pretraining                                               â”‚
â”‚     â””â”€â†’ ä» scratch è®­ç»ƒ GPT æ¨¡å‹                               â”‚
â”‚     â””â”€â†’ æ”¯æŒ scaling laws å®éªŒ                                â”‚
â”‚     â””â”€â†’ è‡ªåŠ¨è®¡ç®—æœ€ä¼˜è¶…å‚ï¼ˆåŸºäº depthï¼‰                        â”‚
â”‚                                                               â”‚
â”‚  3. Finetuning (SFT)                                          â”‚
â”‚     â””â”€â†’ ç›‘ç£å¾®è°ƒï¼Œå­¦ä¹ å¯¹è¯æ ¼å¼                                â”‚
â”‚                                                               â”‚
â”‚  4. Reinforcement Learning (å®éªŒæ€§)                            â”‚
â”‚     â””â”€â†’ GRPO on GSM8K                                         â”‚
â”‚                                                               â”‚
â”‚  5. Inference                                                 â”‚
â”‚     â””â”€â†’ æ–‡æœ¬ç”Ÿæˆã€é‡‡æ ·ç­–ç•¥                                    â”‚
â”‚                                                               â”‚
â”‚  6. Web UI                                                    â”‚
â”‚     â””â”€â†’ ChatGPT-style èŠå¤©ç•Œé¢                                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ä»£ç ç»“æ„ï¼ˆæºç çº§ï¼‰

```
nanochat/
â”œâ”€â”€ nanochat/                    # æ ¸å¿ƒåº“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                 # GPT æ¨¡å‹å®šä¹‰ï¼ˆ~500 è¡Œï¼‰
â”‚   â”œâ”€â”€ optimizer.py             # ä¼˜åŒ–å™¨é…ç½®
â”‚   â”œâ”€â”€ data.py                  # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ tokenizer.py             # Tokenizer
â”‚   â””â”€â”€ utils.py                 # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                     # è®­ç»ƒ/æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ base_train.py            # åŸºç¡€è®­ç»ƒï¼ˆ~800 è¡Œï¼‰
â”‚   â”œâ”€â”€ chat_web.py              # Web UI
â”‚   â”œâ”€â”€ eval.py                  # è¯„ä¼°
â”‚   â”œâ”€â”€ chat_rl.py               # RL å®éªŒ
â”‚   â””â”€â”€ train_tokenizer.py       # Tokenizer è®­ç»ƒ
â”‚
â”œâ”€â”€ runs/                        # é¢„è®¾é…ç½®
â”‚   â”œâ”€â”€ speedrun.sh              # GPT-2 speedrunï¼ˆæ ¸å¿ƒï¼ï¼‰
â”‚   â”œâ”€â”€ scaling_laws.sh          # Scaling å®éªŒ
â”‚   â””â”€â”€ miniseries.sh            # æ¨¡å‹ç³»åˆ—
â”‚
â””â”€â”€ dev/                         # å¼€å‘æ–‡æ¡£
    â”œâ”€â”€ LEADERBOARD.md           # Speedrun æ’è¡Œæ¦œ
    â””â”€â”€ ...
```

---

## ğŸ’¡ æ ¸å¿ƒè®¾è®¡æ€æƒ³

### 1. "Single Dial" å¤æ‚åº¦æ§åˆ¶ â­

**NanoChat æœ€å·§å¦™çš„è®¾è®¡ï¼šä¸€ä¸ªå‚æ•°æ§åˆ¶ä¸€åˆ‡**

```bash
# åªéœ€è¦è®¾ç½® depthï¼Œå…¶ä»–è¶…å‚è‡ªåŠ¨è®¡ç®—
--depth=26  # GPT-2 çº§åˆ«ï¼ˆ~1.6Bï¼‰
--depth=12  # GPT-1 çº§åˆ«ï¼ˆ~350Mï¼‰
--depth=6   # ç©å…·æ¨¡å‹
```

**è‡ªåŠ¨è®¡ç®—çš„è¶…å‚ï¼š**

```python
# ä¼ªä»£ç ï¼ˆåŸºäº nanochat/scripts/base_train.pyï¼‰

def compute_hyperparams(depth):
    """
    åŸºäº depth è‡ªåŠ¨è®¡ç®—æ‰€æœ‰è¶…å‚æ•°
    éµå¾ª scaling laws
    """
    # æ¨¡å‹æ¶æ„
    width = int(4 * depth * 64)  # éšè—å±‚ç»´åº¦
    num_heads = width // 64       # æ³¨æ„åŠ›å¤´æ•°
    num_layers = depth            # Transformer å±‚æ•°
    
    # è®­ç»ƒé…ç½®
    batch_size = compute_optimal_batch_size(width, depth)
    learning_rate = 0.002 * (width / 768) ** -0.5
    warmup_steps = int(0.01 * total_steps)
    weight_decay = 0.1
    
    # è®¡ç®—é‡ä¼°è®¡
    total_flops = estimate_flops(width, depth, batch_size, total_steps)
    
    return {
        'width': width,
        'num_heads': num_heads,
        'num_layers': num_layers,
        'batch_size': batch_size,
        'learning_rate': learning_rate,
        'warmup_steps': warmup_steps,
        'weight_decay': weight_decay,
        'total_flops': total_flops,
    }
```

**è®¾è®¡å“²å­¦ï¼š**
> "è®©ç ”ç©¶è€…ä¸“æ³¨äºæƒ³æ³•ï¼Œè€Œä¸æ˜¯è°ƒå‚"

**å¯¹ä½ çš„å¯å‘ï¼š**
- è®¾è®¡å®éªŒæ—¶ï¼Œå°½é‡å‡å°‘è‡ªç”±å‚æ•°
- åŸºäºç†è®ºï¼ˆscaling lawsï¼‰è‡ªåŠ¨è®¡ç®—é…ç½®
- å¯ä»¥å¿«é€Ÿè¿­ä»£ï¼ˆæ”¹ä¸€ä¸ªå‚æ•°å³å¯ï¼‰

---

### 2. Compute-Optimal Training

**åŸºäº Chinchilla scaling lawsï¼Œè‡ªåŠ¨è®¡ç®—æœ€ä¼˜é…ç½®ï¼š**

```
æ€»è®¡ç®—é‡ = f(depth, width, sequence_length, batch_size, steps)

ç»™å®šé¢„ç®— â†’ è‡ªåŠ¨åˆ†é… â†’ æœ€ä¼˜æ€§èƒ½
```

**å®é™…æ•ˆæœï¼š**
- $72 è®­ç»ƒ GPT-2 çº§åˆ«ï¼ˆ1.6Bï¼‰
- æ¯” 2025 å¹´ OpenAI èŠ±è´¹ï¼ˆ$43,000ï¼‰ä¾¿å®œ 600 å€

**Scaling Laws å…¬å¼ï¼ˆç®€åŒ–ï¼‰ï¼š**

```python
# åŸºäº Kaplan et al. (2020) å’Œ Chinchilla

def chinchilla_optimal_params(compute_budget):
    """
    ç»™å®šè®¡ç®—é¢„ç®—ï¼Œè®¡ç®—æœ€ä¼˜çš„æ¨¡å‹å¤§å°å’Œè®­ç»ƒ token æ•°
    """
    # Chinchilla å‘ç°ï¼š
    # - æ¨¡å‹å¤§å°å’Œè®­ç»ƒ token æ•°åº”è¯¥æŒ‰æ¯”ä¾‹ç¼©æ”¾
    # - N_optimal âˆ C^0.5
    # - D_optimal âˆ C^0.5
    
    C = compute_budget  # FLOPs
    
    # æœ€ä¼˜æ¨¡å‹å¤§å°ï¼ˆå‚æ•°ï¼‰
    N_optimal = (C / (6 * 1.2)) ** 0.5
    
    # æœ€ä¼˜è®­ç»ƒ token æ•°
    D_optimal = (C * 1.2 / 6) ** 0.5
    
    return N_optimal, D_optimal
```

**å¯¹ä½ çš„å¯å‘ï¼š**
- è®¾è®¡å®éªŒæ—¶ç”¨ scaling laws æŒ‡å¯¼é…ç½®
- é¿å…æµªè´¹è®¡ç®—èµ„æº
- å¯ä»¥é¢„æµ‹éœ€è¦å¤šå°‘è®¡ç®—é‡

---

### 3. Speedrun æ–‡åŒ– â­

**å—æ¸¸æˆ speedrun å¯å‘ï¼Œå»ºç«‹è®­ç»ƒæ—¶é—´æ’è¡Œæ¦œï¼š**

| æ’å | æ—¶é—´ | CORE Score | æ—¥æœŸ | è´¡çŒ®è€… |
|------|------|------------|------|--------|
| 0 | - | 0.2565 | 2025 | OpenAI (GPT-2 åŸæ¨¡å‹) |
| 1 | 3.04h | 0.2585 | Jan 29 2026 | @karpathy |
| 2 | 2.91h | 0.2578 | Feb 2 2026 | @karpathy |
| 3 | 2.76h | 0.2602 | Feb 5 2026 | @karpathy |

**ç›®æ ‡ï¼š** ä¸æ–­åˆ·æ–°"è®­ç»ƒåˆ° GPT-2 èƒ½åŠ›"çš„æœ€çŸ­æ—¶é—´

**Speedrun è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š**

```bash
#!/bin/bash
# runs/speedrun.sh

# 1. è®­ç»ƒ tokenizerï¼ˆå¦‚æœéœ€è¦ï¼‰
python -m scripts.train_tokenizer

# 2. é¢„è®­ç»ƒï¼ˆ~3 å°æ—¶ï¼‰
torchrun --standalone --nproc_per_node=8 -m scripts.base_train \
    --depth=26 \
    --run="speedrun" \
    --model-tag="gpt2"

# 3. å¾®è°ƒï¼ˆSFTï¼‰
python -m scripts.finetune \
    --checkpoint="checkpoints/speedrun/gpt2.pt"

# 4. å¯åŠ¨ Web UI
python -m scripts.chat_web
```

**å¯¹ä½ çš„å¯å‘ï¼š**
- å»ºç«‹å¿«é€Ÿå®éªŒå¾ªç¯
- è®¾å®šæ˜ç¡®çš„ç›®æ ‡ï¼ˆå¦‚"3 å°æ—¶å‡ºç»“æœ"ï¼‰
- å¯ä»¥å»ºç«‹è‡ªå·±çš„"speedrun"åŸºå‡†

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚ï¼ˆæºç çº§ï¼‰

### 1. æ¨¡å‹æ¶æ„

**GPT æ¨¡å‹å®šä¹‰ï¼ˆç®€åŒ–è‡ª nanochat/model.pyï¼‰ï¼š**

```python
import torch
import torch.nn as nn

class GPT(nn.Module):
    def __init__(self, depth, width, num_heads, vocab_size, seq_len):
        super().__init__()
        
        # Token embedding
        self.token_embedding = nn.Embedding(vocab_size, width)
        
        # Position embedding
        self.position_embedding = nn.Embedding(seq_len, width)
        
        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(width, num_heads)
            for _ in range(depth)
        ])
        
        # Layer norm
        self.ln_f = nn.LayerNorm(width)
        
        # Language model head
        self.lm_head = nn.Linear(width, vocab_size, bias=False)
        
        # Weight tying (å¯é€‰)
        self.lm_head.weight = self.token_embedding.weight
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, x, positions=None):
        B, T = x.shape
        
        # Get embeddings
        tok_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(positions) if positions is not None else 0
        
        x = tok_emb + pos_emb
        
        # Apply transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # Final layer norm
        x = self.ln_f(x)
        
        # Project to vocabulary
        logits = self.lm_head(x)
        
        return logits


class TransformerBlock(nn.Module):
    def __init__(self, width, num_heads):
        super().__init__()
        
        # Layer norms
        self.ln_1 = nn.LayerNorm(width)
        self.ln_2 = nn.LayerNorm(width)
        
        # Self-attention
        self.attn = nn.MultiheadAttention(width, num_heads, batch_first=True)
        
        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(width, 4 * width),
            nn.GELU(),
            nn.Linear(4 * width, width),
        )
    
    def forward(self, x):
        # Self-attention with residual
        attn_output, _ = self.attn(x, x, x, need_weights=False)
        x = x + attn_output
        x = self.ln_1(x)
        
        # MLP with residual
        mlp_output = self.mlp(x)
        x = x + mlp_output
        x = self.ln_2(x)
        
        return x
```

**å…³é”®è®¾è®¡é€‰æ‹©ï¼š**
- æ ‡å‡†çš„ decoder-only Transformer
- æƒé‡ç»‘å®šï¼ˆtoken embedding â†” lm_headï¼‰
- ä½ç½®ç¼–ç ï¼šå­¦ä¹ å¼ï¼ˆé RoPEï¼‰
- æ¿€æ´»å‡½æ•°ï¼šGELU
- MLP æ¯”ä¾‹ï¼š4x

---

### 2. è®­ç»ƒä¼˜åŒ–

#### A. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# è‡ªåŠ¨ä½¿ç”¨ FP8/FP16/FP32 æ··åˆç²¾åº¦

# åœ¨ base_train.py ä¸­
dtype = torch.float16  # æˆ– torch.bfloat16, torch.float8_e4m3fn

# ä½¿ç”¨ GradScaler for FP16
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast(dtype=dtype):
    logits = model(inputs)
    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**å¯¹ä½ çš„å¯å‘ï¼š**
- æ··åˆç²¾åº¦å¯ä»¥èŠ‚çœæ˜¾å­˜ã€åŠ é€Ÿè®­ç»ƒ
- H100 æ”¯æŒ FP8ï¼Œå¯ä»¥è¿›ä¸€æ­¥åŠ é€Ÿ
- æ³¨æ„æ•°å€¼ç¨³å®šæ€§

---

#### B. æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
# èŠ‚çœæ˜¾å­˜ï¼Œæ”¯æŒæ›´å¤§æ¨¡å‹

from torch.utils.checkpoint import checkpoint

class TransformerBlock(nn.Module):
    def forward(self, x):
        # ä½¿ç”¨ gradient checkpointing
        if self.use_checkpointing:
            x = checkpoint(self._forward_impl, x)
        else:
            x = self._forward_impl(x)
        return x
```

**æ•ˆæœï¼š**
- æ˜¾å­˜èŠ‚çœï¼š~50%
- é€Ÿåº¦æŸå¤±ï¼š~20%
- å¯ä»¥è®­ç»ƒæ›´å¤§æ¨¡å‹

---

#### C. åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# 8Ã—GPU æ•°æ®å¹¶è¡Œ

torchrun --standalone --nproc_per_node=8 -m scripts.base_train \
    --depth=26 \
    --run="speedrun"
```

**å®ç°ç»†èŠ‚ï¼š**
```python
# ä½¿ç”¨ DDP (DistributedDataParallel)

import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
dist.init_process_group("nccl")
local_rank = int(os.environ["LOCAL_RANK"])

# åˆ›å»ºæ¨¡å‹
model = GPT(...)
model = model.to(local_rank)
model = DDP(model, device_ids=[local_rank])

# æ•°æ®é‡‡æ ·å™¨ï¼ˆç¡®ä¿æ¯ä¸ª GPU çœ‹åˆ°ä¸åŒæ•°æ®ï¼‰
sampler = DistributedSampler(dataset)
dataloader = DataLoader(dataset, sampler=sampler)
```

---

### 3. æ•°æ®ç®¡é“

#### Tokenizer è®­ç»ƒ

```python
# è®­ç»ƒè‡ªå®šä¹‰ BPE tokenizer

from tokenizers import Tokenizer, models, trainers

# åˆ›å»º tokenizer
tokenizer = Tokenizer(models.BPE())

# è®­ç»ƒ
trainer = trainers.BpeTrainer(
    vocab_size=50257,
    min_frequency=2,
    special_tokens=["<|endoftext|>", "<|fim_prefix|>", "<|fim_middle|>", "<|fim_suffix|>"]
)

tokenizer.train(files=data_files, trainer=trainer)

# ä¿å­˜
tokenizer.save("tokenizer.json")
```

**è®¾è®¡è€ƒè™‘ï¼š**
- vocab_size=50,257ï¼ˆä¸ GPT-2 å…¼å®¹ï¼‰
- æ”¯æŒ FIMï¼ˆFill-In-the-Middleï¼‰
- å¯ä»¥æ‰©å±•å¤šè¯­è¨€

---

## ğŸ“Š Performance åˆ†æ

### è®­ç»ƒé€Ÿåº¦

| é…ç½® | GPU | æ—¶é—´ | æˆæœ¬ |
|------|-----|------|------|
| GPT-2 (1.6B) | 8Ã—H100 | ~3h | ~$72 |
| GPT-2 (1.6B) | 8Ã—A100 | ~5h | ~$60 |
| GPT-1 (350M) | 8Ã—H100 | ~30min | ~$12 |

### æ¨¡å‹æ€§èƒ½

**CORE Metric (DCLM åŸºå‡†)ï¼š**

| æ¨¡å‹ | CORE Score | å¯¹æ¯” |
|------|------------|------|
| GPT-2 (åŸ) | 0.2565 | baseline |
| NanoChat d26 | 0.2602 | **è¶…è¶Š GPT-2** |
| NanoChat d12 | ~0.35 | GPT-1 çº§åˆ« |

**æ³¨ï¼š** CORE score è¶Šä½è¶Šå¥½

---

## ğŸ“ å¯¹ Efficient AI ç ”ç©¶çš„å¯å‘

### 1. æˆæœ¬æ„è¯† (Cost-Awareness) â­

**Karpathy çš„å“²å­¦ï¼š**
> "å¦‚æœè®­ç»ƒå¤ªè´µï¼Œä½ å°±ä¸ä¼šåšè¶³å¤Ÿçš„å®éªŒ"

**åº”ç”¨åˆ°ä½ çš„ç ”ç©¶ï¼š**
- è®¾è®¡å®éªŒæ—¶è€ƒè™‘æˆæœ¬
- ç”¨å°æ¨¡å‹å¿«é€ŸéªŒè¯æƒ³æ³•
- æŠ¥å‘Šç»“æœæ—¶åŒ…å«æˆæœ¬ä¿¡æ¯

**å…·ä½“åšæ³•ï¼š**
```
1. ä¼°ç®—æ¯ä¸ªå®éªŒçš„æˆæœ¬
   - GPU æ—¶é—´ Ã— å•ä»·
   - æ€»é¢„ç®—åˆ†é…

2. ä¼˜å…ˆåšä½æˆæœ¬å®éªŒ
   - å°æ¨¡å‹ï¼ˆdepth=6/12ï¼‰
   - å°‘æ­¥æ•°ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

3. å†åšå¤§å®éªŒ
   - ç¡®è®¤æƒ³æ³•æœ‰æ•ˆ
   - ç”¨å®Œæ•´é…ç½®
```

---

### 2. å¯å¤ç°æ€§ (Reproducibility)

**NanoChat çš„åšæ³•ï¼š**
- å•è„šæœ¬å¤ç°ï¼ˆspeedrun.shï¼‰
- å›ºå®šéšæœºç§å­
- è¯¦ç»†è®°å½•è¶…å‚

**åº”ç”¨åˆ°ä½ çš„ç ”ç©¶ï¼š**
```bash
# ä½ çš„å®éªŒè„šæœ¬åº”è¯¥åƒè¿™æ ·

#!/bin/bash
# runs/my-experiment.sh

# å›ºå®šéšæœºç§å­
export PYTHONHASHSEED=42
RANDOM=42
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# è®°å½•é…ç½®
cat > config.json << EOF
{
    "depth": 12,
    "width": 768,
    "learning_rate": 0.0003,
    "batch_size": 64,
    ...
}
EOF

# è¿è¡Œå®éªŒ
python -m scripts.base_train --config=config.json

# ä¿å­˜ç»“æœ
cp logs/*.json results/my-experiment/
```

---

### 3. å¿«é€Ÿè¿­ä»£ (Fast Iteration) â­

**NanoChat çš„è¿­ä»£å¾ªç¯ï¼š**

```
æ”¹ä»£ç  â†’ è·‘ d12 (5min) â†’ çœ‹ wandb â†’ é‡å¤
```

**åº”ç”¨åˆ°ä½ çš„ç ”ç©¶ï¼š**

```python
# å»ºç«‹å¿«é€Ÿå®éªŒ pipeline

def quick_experiment(idea_name, modification):
    """
    å¿«é€ŸéªŒè¯ä¸€ä¸ªæƒ³æ³•
    """
    # 1. ç”¨å°æ¨¡å‹ï¼ˆdepth=12ï¼‰
    config = {
        'depth': 12,
        'run': f'quick-{idea_name}',
    }
    
    # 2. åº”ç”¨ä¿®æ”¹
    apply_modification(config, modification)
    
    # 3. è¿è¡Œï¼ˆ~5 åˆ†é’Ÿï¼‰
    results = run_training(config)
    
    # 4. è®°å½•
    log_results(idea_name, results)
    
    return results

# ä½¿ç”¨
for idea in [idea1, idea2, idea3]:
    quick_experiment(idea.name, idea.modification)
```

---

### 4. ç«¯åˆ°ç«¯ç†è§£ (End-to-End Understanding)

**NanoChat è¦†ç›–å…¨æµç¨‹ï¼š**
- Tokenizer â†’ Pretrain â†’ SFT â†’ RL â†’ Inference â†’ UI

**åº”ç”¨åˆ°ä½ çš„ç ”ç©¶ï¼š**
- ä¸è¦åªå…³æ³¨å•ä¸€ç¯èŠ‚
- ç†è§£æ•´ä¸ª pipeline çš„ç“¶é¢ˆ
- ç³»ç»Ÿæ€§ä¼˜åŒ–

**ä¾‹å¦‚ï¼Œåš Efficient AIï¼š**
```
1. ç†è§£è®­ç»ƒç“¶é¢ˆ
   - æ•°æ®åŠ è½½ï¼Ÿ
   - å‰å‘ä¼ æ’­ï¼Ÿ
   - åå‘ä¼ æ’­ï¼Ÿ

2. ç†è§£æ¨ç†ç“¶é¢ˆ
   - æ˜¾å­˜ï¼Ÿ
   - è®¡ç®—ï¼Ÿ
   - é€šä¿¡ï¼Ÿ

3. é’ˆå¯¹æ€§ä¼˜åŒ–
   - è®­ç»ƒï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦
   - æ¨ç†ï¼šé‡åŒ–ã€å‰ªæã€KV cache
```

---

## ğŸ› ï¸ å¦‚ä½•ç”¨äºä½ çš„ç ”ç©¶

### åœºæ™¯ 1ï¼šå¿«é€ŸéªŒè¯ Efficient AI æƒ³æ³•

```bash
# 1. å…‹éš† NanoChat
git clone https://github.com/karpathy/nanochat
cd nanochat

# 2. ä¿®æ”¹æ¨¡å‹ï¼ˆå¦‚æ·»åŠ é‡åŒ–ï¼‰
# ç¼–è¾‘ nanochat/model.py

# 3. ç”¨å°æ¨¡å‹å¿«é€Ÿæµ‹è¯•
python -m scripts.base_train --depth=12 --run="my-quantization"

# 4. çœ‹ç»“æœï¼ˆ~5 åˆ†é’Ÿåï¼‰
# æŸ¥çœ‹ wandb dashboard
```

**æ—¶é—´ï¼š** 30 åˆ†é’Ÿè®¾ç½® + 5 åˆ†é’Ÿè¿è¡Œ = 35 åˆ†é’ŸéªŒè¯ä¸€ä¸ªæƒ³æ³•

---

### åœºæ™¯ 2ï¼šScaling Laws å®éªŒ

```bash
# è¿è¡Œé¢„è®¾çš„ scaling å®éªŒ
bash runs/scaling_laws.sh

# åˆ†æä¸åŒè§„æ¨¡ä¸‹çš„æ•ˆç‡/æ€§èƒ½æƒè¡¡
```

**è¾“å‡ºï¼š**
- ä¸åŒ depth çš„ loss æ›²çº¿
- FLOPs vs Performance
- æœ€ä¼˜é…ç½®å»ºè®®

---

### åœºæ™¯ 3ï¼šéƒ¨ç½²ç ”ç©¶

```bash
# è®­ç»ƒå®Œæˆåç›´æ¥æµ‹è¯•æ¨ç†
python -m scripts.chat_web

# æµ‹é‡å»¶è¿Ÿã€ååé‡
# æµ‹è¯•é‡åŒ–/å‰ªææ•ˆæœ
```

**å¯ä»¥åšçš„å®éªŒï¼š**
- INT8 é‡åŒ–åçš„å»¶è¿Ÿå˜åŒ–
- å‰ªæåçš„è´¨é‡æŸå¤±
- ä¸åŒ batch size çš„ååé‡

---

## ğŸ“š å­¦ä¹ è·¯å¾„å»ºè®®

### ç¬¬ 1 å‘¨ï¼šç†Ÿæ‚‰é¡¹ç›®

**Day 1-2: é˜…è¯»æ–‡æ¡£**
- README.md
- dev/LEADERBOARD.md
- Discussion å¸–å­

**Day 3-4: è¿è¡Œ speedrun**
```bash
bash runs/speedrun.sh
```

**Day 5-7: ç†è§£ä»£ç **
- nanochat/model.py
- scripts/base_train.py
- ç”»å‡ºæ•°æ®æµå›¾

---

### ç¬¬ 2 å‘¨ï¼šä¿®æ”¹å®éªŒ

**Day 1-2: å°æ”¹åŠ¨**
- æ”¹å­¦ä¹ ç‡
- æ”¹æ·±åº¦
- çœ‹ wandb å˜åŒ–

**Day 3-4: ä¸­ç­‰æ”¹åŠ¨**
- æ·»åŠ æ–°çš„æ¿€æ´»å‡½æ•°
- æ”¹ä½ç½®ç¼–ç 
- å¯¹æ¯”æ•ˆæœ

**Day 5-7: å¤§æ”¹åŠ¨**
- æ·»åŠ é‡åŒ–
- æ·»åŠ å‰ªæ
- å®Œæ•´å®éªŒ

---

### ç¬¬ 3 å‘¨ï¼šäº§å‡ºç»“æœ

**Day 1-3: ç³»ç»Ÿå®éªŒ**
- è®¾è®¡å®éªŒæ–¹æ¡ˆ
- è·‘å¤šä¸ªé…ç½®
- åˆ†æç»“æœ

**Day 4-5: å†™æŠ¥å‘Š/è®ºæ–‡**
- è®°å½•æ–¹æ³•
- å¯¹æ¯” baseline
- å¾—å‡ºç»“è®º

**Day 6-7: å¼€æºä»£ç **
- fork nanochat
- æäº¤ PR
- ç¤¾åŒºåé¦ˆ

---

## ğŸ”— èµ„æºé“¾æ¥

- **GitHub:** https://github.com/karpathy/nanochat
- **Discussion:** https://github.com/karpathy/nanochat/discussions
- **DeepWiki:** https://deepwiki.com/karpathy/nanochat (AI ä»£ç é—®ç­”)
- **Discord:** #nanochat channel

---

## ğŸ’­ ä¸ªäººè¯„ä»·

### ä¼˜ç‚¹

1. **æç®€è®¾è®¡** â€” ä»£ç æ¸…æ™°ï¼Œæ˜“äºç†è§£
2. **æˆæœ¬ä½å»‰** â€” å­¦ç”Ÿ/ç ”ç©¶è€…å¯è´Ÿæ‹…
3. **å…¨æ ˆè¦†ç›–** â€” ä»è®­ç»ƒåˆ°éƒ¨ç½²
4. **ç¤¾åŒºé©±åŠ¨** â€” speedrun æ’è¡Œæ¦œæ¿€åŠ±è´¡çŒ®
5. **æ•™è‚²ä»·å€¼** â€” å­¦ä¹  LLM çš„ç»ä½³ææ–™

### å±€é™

1. **åŠŸèƒ½æœ‰é™** â€” åªæ”¯æŒåŸºç¡€ GPT æ¶æ„
2. **æ€§èƒ½ä¸Šé™** â€” ä¸é€‚åˆ SOTA ç ”ç©¶
3. **æ–‡æ¡£ä¸è¶³** â€” éƒ¨åˆ†åŠŸèƒ½éœ€è¦è¯»ä»£ç 
4. **RL å®éªŒæ€§** â€” å¼ºåŒ–å­¦ä¹ éƒ¨åˆ†ä¸æˆç†Ÿ

### æ¨èäººç¾¤

- âœ… LLM åˆå­¦è€…ï¼ˆå­¦ä¹ å…¨æ ˆæµç¨‹ï¼‰
- âœ… Efficient AI ç ”ç©¶è€…ï¼ˆå¿«é€ŸéªŒè¯æƒ³æ³•ï¼‰
- âœ… æ•™è‚²ç”¨é€”ï¼ˆæ•™å­¦æ¼”ç¤ºï¼‰
- âŒ SOTA è¿½é€è€…ï¼ˆç”¨æ›´å¤§çš„æ¡†æ¶ï¼‰
- âŒ ç”Ÿäº§éƒ¨ç½²ï¼ˆç”¨æ›´æˆç†Ÿçš„å·¥å…·ï¼‰

---

## ğŸ“¬ æ€»ç»“

**NanoChat çš„æ ¸å¿ƒä»·å€¼ï¼š**

> "è®© LLM è®­ç»ƒå˜å¾—åƒæ­ç§¯æœ¨ä¸€æ ·ç®€å•"

**å¯¹ä½ çš„ç ”ç©¶ï¼š**

1. **å­¦ä¹ å·¥å…·** â€” ç†è§£ LLM è®­ç»ƒå…¨æµç¨‹
2. **å®éªŒå¹³å°** â€” å¿«é€ŸéªŒè¯ Efficient AI æƒ³æ³•
3. **çµæ„Ÿæ¥æº** â€” è®¾è®¡è‡ªå·±çš„"æç®€æ¡†æ¶"

**è¡ŒåŠ¨å»ºè®®ï¼š**

1. è¿™å‘¨å°±å…‹éš†é¡¹ç›®ï¼Œè·‘ä¸€æ¬¡ speedrun
2. å°è¯•ä¸€ä¸ªå°ä¿®æ”¹ï¼ˆå¦‚æ”¹æ¿€æ´»å‡½æ•°ï¼‰
3. æ€è€ƒå¦‚ä½•åº”ç”¨åˆ°ä½ çš„ Efficient AI ç ”ç©¶

---

*è¿”å› [00-daily-updates.md](00-daily-updates.md)*
