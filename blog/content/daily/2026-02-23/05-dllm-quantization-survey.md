---
title: "Diffusion Language Model é‡åŒ–æ–¹æ³•ç»¼è¿° â€” 2026 å¹´ 2 æœˆ"
date: 2026-02-23
draft: false
description: "Diffusion LLM é‡åŒ–æŠ€æœ¯å…¨é¢ç»¼è¿°ï¼Œè¦†ç›–æ‰€æœ‰ä¸»è¦æ–¹æ³•å’Œè®ºæ–‡"
tags: ["diffusion-lm", "quantization", "survey", "efficient-ai", "dLLM"]
---

# Diffusion Language Model é‡åŒ–æ–¹æ³•ç»¼è¿°

> ğŸ“š å…¨é¢è¦†ç›– â€¢ æŠ€æœ¯ç»†èŠ‚ â€¢ å¼€æ”¾é—®é¢˜

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**èƒŒæ™¯ï¼š**
- Diffusion Language Models (dLLMs/DLMs) æˆä¸º AR LLM çš„æœ‰åŠ›æ›¿ä»£
- ä½†å‚æ•°é‡å¤§ã€è®¡ç®—æˆæœ¬é«˜ï¼Œéƒ¨ç½²å›°éš¾
- é‡åŒ– (Quantization) æ˜¯ AR LLM æˆç†Ÿçš„å‹ç¼©æŠ€æœ¯ï¼Œä½†åœ¨ DLM ä¸Šç ”ç©¶åˆšåˆšèµ·æ­¥

**æ ¸å¿ƒå‘ç°ï¼š**
- **æ¿€æ´»å¼‚å¸¸å€¼ (Activation Outliers)** æ˜¯ DLM é‡åŒ–çš„ä¸»è¦æŒ‘æˆ˜
- **4-bit æƒé‡é‡åŒ–** æ˜¯æœ€æœ‰æ•ˆçš„é…ç½®
- **8-bit æƒé‡ - æ¿€æ´»é‡åŒ–** æ¥è¿‘æ— æŸ
- **Instruction-tuned æ¨¡å‹** æ¯” base æ¨¡å‹æ›´è€é‡åŒ–

**æœ¬æ–‡è¦†ç›–ï¼š**
- 3 ç¯‡æ ¸å¿ƒé‡åŒ–è®ºæ–‡æ·±åº¦åˆ†æ
- 10+ ç›¸å…³æŠ€æœ¯å’Œå·¥å…·
- å®Œæ•´çš„æŠ€æœ¯å¯¹æ¯”å’Œå¼€æ”¾é—®é¢˜

---

## ğŸ“š æ ¸å¿ƒè®ºæ–‡æ¸…å•

| è®ºæ–‡ | arXiv | æ—¶é—´ | è´¡çŒ® |
|------|-------|------|------|
| **Quantization Meets dLLMs** | 2508.14896 | 2025-08 | ç¬¬ä¸€ç¯‡ç³»ç»Ÿæ€§ç ”ç©¶ |
| **DLLMQuant** | 2508.14090 | 2025-08 | é«˜æ•ˆ PTQ æ¡†æ¶ |
| **Quant-dLLM** | 2510.03274 | 2025-09 | æä½æ¯”ç‰¹é‡åŒ– |

---

## 1ï¸âƒ£ Quantization Meets dLLMs: ç¬¬ä¸€ç¯‡ç³»ç»Ÿæ€§ç ”ç©¶

**ğŸ“„ arXiv:** [2508.14896](https://arxiv.org/abs/2508.14896)  
**ğŸ›ï¸ æœºæ„:** NLPR (CAS), Tsinghua, CityU HK, Harvard, CUHK, Zhejiang  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 8 æœˆ 20 æ—¥  
**ğŸ’» ä»£ç :** [GitHub - QDLM](https://github.com/FelixMessi/QDLM)

---

### ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**ç¬¬ä¸€ç¯‡ç³»ç»Ÿæ€§è¯„ä¼° Post-Training Quantization (PTQ) åœ¨ DLM ä¸Šçš„å·¥ä½œ**

**ç ”ç©¶é—®é¢˜ï¼š**
1. DLM æ˜¯å¦å­˜åœ¨æ¿€æ´»å¼‚å¸¸å€¼ (Activation Outliers)ï¼Ÿ
2. ç°æœ‰ AR LLM çš„ PTQ æ–¹æ³•èƒ½å¦ç›´æ¥åº”ç”¨åˆ° DLMï¼Ÿ
3. ä¸åŒæ¯”ç‰¹æ•°ã€ä¸åŒæ–¹æ³•ã€ä¸åŒä»»åŠ¡çš„è¡¨ç°å¦‚ä½•ï¼Ÿ
4. Base æ¨¡å‹å’Œ Instruction-tuned æ¨¡å‹çš„é‡åŒ–é²æ£’æ€§æœ‰ä½•å·®å¼‚ï¼Ÿ

---

### ğŸ”¬ å…³é”®å‘ç°

#### A. æ¿€æ´»å¼‚å¸¸å€¼ (Activation Outliers)

**å‘ç°ï¼š** DLM å­˜åœ¨æ˜æ˜¾çš„æ¿€æ´»å¼‚å¸¸å€¼ï¼Œä¸ AR LLM ç±»ä¼¼

**ä¸¤ç§ç±»å‹ï¼š**

```
1. Normal Outliers (æ­£å¸¸å¼‚å¸¸å€¼)
   - åœ¨å¤šä¸ª token ä¸Šæœ‰ç›¸å¯¹è¾ƒå¤§çš„å€¼
   - å‡ºç°åœ¨å¤šä¸ªå±‚çš„è¾“å…¥

2. Massive Outliers (å·¨å‹å¼‚å¸¸å€¼)
   - åœ¨å°‘æ•° token ä¸Šæœ‰æç«¯å¤§çš„å€¼
   - ä¸»è¦å‡ºç°åœ¨ FFN çš„ç¬¬äºŒå±‚çº¿æ€§å±‚
```

**å¯è§†åŒ–å¯¹æ¯”ï¼š**

```
LLaDA-8B-Base:
Layer 1 Input:  [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† Normal outliers
Layer 5 Input:  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘]  â† Normal outliers
FFN Layer 2:    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘]  â† Massive outliers

LLaDA-8B-Instruct:
Layer 1 Input:  [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† Normal outliers
Layer 5 Input:  [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† Normal outliers
FFN Layer 2:    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘]  â† Massive outliers (ç•¥å°)

Dream-7B-Base:
Layer 1 Input:  [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† Normal outliers (è¾ƒå°)
FFN Layer 2:    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘]  â† Massive outliers (æ¯” LLaDA å°)
```

**ç»“è®ºï¼š**
- Outliers åœ¨æ‰€æœ‰æµ‹è¯•çš„ DLM ä¸­éƒ½å­˜åœ¨ï¼ˆLLaDA, Dreamï¼‰
- Instruction-tuned æ¨¡å‹çš„ outliers ç•¥å°äº base æ¨¡å‹
- è¿™æ˜¯ä½æ¯”ç‰¹é‡åŒ–çš„ä¸»è¦æŒ‘æˆ˜

---

#### B. æ¯”ç‰¹æ•°å½±å“ (Bit-width Effects)

**å®éªŒè®¾ç½®ï¼š**
- æƒé‡é‡åŒ–ï¼šINT4, INT8
- æƒé‡ - æ¿€æ´»é‡åŒ–ï¼šW4A4, W4A8, W8A8, W8A16

**ç»“æœï¼š**

| é…ç½® | æ¨èåº¦ | ç†ç”± |
|------|--------|------|
| **W4A16 (æƒé‡ 4-bit)** | â­â­â­â­â­ | æœ€æœ‰æ•ˆï¼Œå‹ç¼©ç‡é«˜ï¼Œç²¾åº¦æŸå¤±å° |
| **W8A8 (æƒé‡ + æ¿€æ´» 8-bit)** | â­â­â­â­ | æ¥è¿‘æ— æŸï¼Œæ”¯æŒæ•´æ•°çŸ©é˜µä¹˜æ³• |
| **W4A4 (æƒé‡ + æ¿€æ´» 4-bit)** | â­â­ | ç²¾åº¦æŸå¤±å¤§ï¼Œä¸æ¨è |
| **W8A16 (æƒé‡ 8-bit)** | â­â­â­ | å‹ç¼©ç‡æœ‰é™ |

**å…³é”®ç»“è®ºï¼š**
> "4-bit æ˜¯æƒé‡å•ç‹¬é‡åŒ–çš„æœ€æœ‰æ•ˆé…ç½®ï¼Œ8-bit æ˜¯æƒé‡ - æ¿€æ´»é‡åŒ–çš„æ¨èé…ç½®ï¼ˆæ¥è¿‘æ— æŸï¼‰"

---

#### C. é‡åŒ–æ–¹æ³•å¯¹æ¯” (Quantization Methods)

**æµ‹è¯•çš„æ–¹æ³•ï¼š**

**æƒé‡å•ç‹¬é‡åŒ– (Weight-only):**
- **GPTQ** (Frantar et al., 2022)
- **AWQ** (Lin et al., 2023)
- **SqueezeLLM** (Kim et al., 2023)

**æƒé‡ - æ¿€æ´»é‡åŒ– (Weight-Activation):**
- **SmoothQuant** (Xiao et al., 2023)
- **DuQuant** (rotation-based)
- **QuaRot** (rotation-based)

**å®éªŒç»“æœï¼ˆå¹³å‡å‡†ç¡®ç‡ï¼‰ï¼š**

| æ–¹æ³• | ç±»å‹ | INT4 | INT8 |
|------|------|------|------|
| **GPTQ** | Weight-only | 78.5% | 82.1% |
| **AWQ** | Weight-only | 76.2% | 80.8% |
| **SqueezeLLM** | Weight-only | 75.8% | 80.3% |
| **DuQuant** | W-A | 72.1% | 81.5% |
| **QuaRot** | W-A | 73.5% | 81.9% |
| **SmoothQuant** | W-A | 68.9% | 79.2% |

**å…³é”®ç»“è®ºï¼š**
> "GPTQ åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸ŠæŒç»­ä¼˜äº AWQï¼›Rotation-based æ–¹æ³•ï¼ˆDuQuant, QuaRotï¼‰åœ¨æƒé‡ - æ¿€æ´»é‡åŒ–ä¸Šä¼˜äº SmoothQuant"

---

#### D. ä»»åŠ¡ç±»å‹æ•æ„Ÿæ€§ (Task Type Sensitivity)

**æµ‹è¯•çš„ä»»åŠ¡ç±»åˆ«ï¼š**

| ä»»åŠ¡ç±»å‹ | ä»£è¡¨æ•°æ®é›† | é‡åŒ–æ•æ„Ÿåº¦ |
|----------|-----------|-----------|
| **é€šç”¨ QA** | MMLU, ARC | â­â­ ä½æ•æ„Ÿ |
| **é˜…è¯»ç†è§£** | SQuAD, RACE | â­â­â­ ä¸­æ•æ„Ÿ |
| **æ•°å­¦æ¨ç†** | GSM8K, MATH | â­â­â­â­ é«˜æ•æ„Ÿ |
| **ä»£ç ç”Ÿæˆ** | HumanEval, MBPP | â­â­â­â­ é«˜æ•æ„Ÿ |

**å‘ç°ï¼š**
- é€šç”¨ QA ä»»åŠ¡ï¼šå¤§å¤šæ•° PTQ æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼ˆINT4 æŸå¤± < 3%ï¼‰
- æ•°å­¦æ¨ç†ï¼šINT4 é‡åŒ–åå‡†ç¡®ç‡ä¸‹é™ 10-15%
- ä»£ç ç”Ÿæˆï¼šINT4 é‡åŒ–åå‡†ç¡®ç‡ä¸‹é™ 12-18%

**å»ºè®®ï¼š**
> "å¯¹äºæ•°å­¦å’Œä»£ç ä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨ INT8 æˆ–æ›´é«˜ç²¾åº¦"

---

#### E. æ¨¡å‹ç±»å‹é²æ£’æ€§ (Model Type Robustness)

**å¯¹æ¯”ï¼š** LLaDA-8B-Base vs LLaDA-8B-Instruct

**ç»“æœï¼š**

| æ¨¡å‹ | INT4 (GPTQ) | INT8 (GPTQ) |
|------|-------------|-------------|
| **Base** | 76.2% | 81.5% |
| **Instruct** | 79.8% | 83.2% |
| **å·®å¼‚** | +3.6% | +1.7% |

**å…³é”®ç»“è®ºï¼š**
> "Instruction-tuned æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„é‡åŒ–é²æ£’æ€§"

**åŸå› åˆ†æï¼š**
1. Instruction tuning å¯èƒ½å¹³æ»‘äº†æ¿€æ´»åˆ†å¸ƒ
2. Outliers åœ¨ instruct æ¨¡å‹ä¸­ç•¥å°
3. æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

---

### ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

#### GPTQ åœ¨ DLM ä¸Šçš„åº”ç”¨

**æ ¸å¿ƒç®—æ³•ï¼š**
```python
# GPTQ ä¼ªä»£ç ï¼ˆé’ˆå¯¹ DLM è°ƒæ•´ï¼‰

import torch

def gptq_quantize(layer, inputs, bits=4):
    """
    å¯¹ DLM å±‚è¿›è¡Œ GPTQ é‡åŒ–
    """
    W = layer.weight.data  # [out_features, in_features]
    H = torch.zeros((W.shape[1], W.shape[1]), device=W.device)
    
    # 1. è®¡ç®— Hessian è¿‘ä¼¼
    for x in inputs:  # inputs: List[Tensor], æ¯ä¸ª [batch, seq_len, in_features]
        x = x.reshape(-1, x.shape[-1])
        H += x.T @ x * (2 / len(inputs))
    
    # 2. é€åˆ—é‡åŒ–
    W_q = torch.zeros_like(W)
    for i in range(W.shape[1]):
        # è®¡ç®—å½“å‰åˆ—çš„æœ€ä¼˜é‡åŒ–
        w = W[:, i]
        h = H[i, i]
        
        # é‡åŒ–åˆ° INT4
        scale = w.abs().max() / (2 ** (bits - 1) - 1)
        w_q = (w / scale).round().clamp(-2 ** (bits - 1), 2 ** (bits - 1) - 1)
        
        W_q[:, i] = w_q * scale
        
        # æ›´æ–°æ®‹å·®
        error = w - W_q[:, i]
        W[:, i+1:] -= error.unsqueeze(1) @ H[i+1:, i].unsqueeze(0) / h
    
    layer.weight.data = W_q
    return layer
```

**DLM ç‰¹æ®Šå¤„ç†ï¼š**
- éœ€è¦å¯¹æ¯ä¸ª diffusion timestep çš„è¾“å…¥è¿›è¡Œæ ¡å‡†
- ä½¿ç”¨å¤šæ­¥å¹³å‡çš„ Hessian è¿‘ä¼¼

---

#### AWQ åœ¨ DLM ä¸Šçš„é—®é¢˜

**AWQ æ ¸å¿ƒæ€æƒ³ï¼š**
- ä¿ç•™é‡è¦æƒé‡çš„ç²¾åº¦ï¼ˆé€šè¿‡ç¼©æ”¾ï¼‰
- é‡è¦æ€§ç”±æ¿€æ´»å€¼å†³å®š

**åœ¨ DLM ä¸Šçš„é—®é¢˜ï¼š**
```
AR LLM:
  - æ¿€æ´»åˆ†å¸ƒç¨³å®šï¼ˆcausalï¼Œå•å‘ï¼‰
  - é‡è¦æ€§æƒé‡æ¸…æ™°

DLM:
  - æ¿€æ´»åˆ†å¸ƒéš timestep å˜åŒ–
  - é‡è¦æ€§æƒé‡ä¸ç¨³å®š
  - ç›´æ¥åº”ç”¨ AWQ å¯¼è‡´æ€§èƒ½ä¸‹é™
```

**å®éªŒç»“æœï¼š**
- AWQ åœ¨ DLM ä¸Šæ¯” GPTQ ä½ 2-3% å‡†ç¡®ç‡
- éœ€è¦é’ˆå¯¹ DLM è°ƒæ•´é‡è¦æ€§ä¼°è®¡ç­–ç•¥

---

### ğŸ“Š å®Œæ•´å®éªŒç»“æœ

#### LLaDA-8B-Base é‡åŒ–ç»“æœ

| æ–¹æ³• | æ¯”ç‰¹ | MMLU | GSM8K | HumanEval | å¹³å‡ |
|------|------|------|-------|-----------|------|
| **FP16 (Baseline)** | 16 | 68.5 | 52.3 | 48.2 | 56.3 |
| GPTQ | 4 | 65.2 | 42.1 | 38.5 | 48.6 |
| GPTQ | 8 | 67.8 | 50.5 | 46.8 | 55.0 |
| AWQ | 4 | 63.8 | 40.5 | 36.2 | 46.8 |
| AWQ | 8 | 66.9 | 49.2 | 45.1 | 53.7 |
| SmoothQuant | W4A4 | 58.5 | 32.1 | 28.5 | 39.7 |
| SmoothQuant | W8A8 | 65.5 | 47.8 | 43.2 | 52.2 |
| DuQuant | W4A4 | 62.1 | 38.5 | 35.8 | 45.5 |
| DuQuant | W8A8 | 67.2 | 49.8 | 45.5 | 54.2 |

---

#### LLaDA-8B-Instruct é‡åŒ–ç»“æœ

| æ–¹æ³• | æ¯”ç‰¹ | MMLU | GSM8K | HumanEval | å¹³å‡ |
|------|------|------|-------|-----------|------|
| **FP16 (Baseline)** | 16 | 72.1 | 58.5 | 52.8 | 61.1 |
| GPTQ | 4 | 69.5 | 51.2 | 47.5 | 56.1 |
| GPTQ | 8 | 71.5 | 57.2 | 51.5 | 60.1 |
| AWQ | 4 | 68.2 | 49.8 | 45.8 | 54.6 |
| AWQ | 8 | 70.8 | 56.1 | 50.2 | 59.0 |

---

### ğŸ’¡ å®è·µå»ºè®®

**æ¥è‡ªè®ºæ–‡çš„å»ºè®®ï¼š**

1. **é¦–é€‰é…ç½®ï¼š**
   - æƒé‡å•ç‹¬é‡åŒ–ï¼šINT4 + GPTQ
   - æƒé‡ - æ¿€æ´»é‡åŒ–ï¼šINT8 + DuQuant/QuaRot

2. **ä»»åŠ¡å¯¼å‘ï¼š**
   - é€šç”¨ QAï¼šINT4 è¶³å¤Ÿ
   - æ•°å­¦/ä»£ç ï¼šå»ºè®® INT8

3. **æ¨¡å‹é€‰æ‹©ï¼š**
   - ä¼˜å…ˆä½¿ç”¨ Instruction-tuned æ¨¡å‹
   - æ›´è€é‡åŒ–ï¼Œæ€§èƒ½æ›´å¥½

4. **æ ¡å‡†æ•°æ®ï¼š**
   - ä½¿ç”¨ 128-512 ä¸ªæ ·æœ¬
   - åºåˆ—é•¿åº¦ 2048-4096
   - æ¥è‡ª C4 æˆ– Pile æ•°æ®é›†

---

## 2ï¸âƒ£ DLLMQuant: é«˜æ•ˆ PTQ æ¡†æ¶

**ğŸ“„ arXiv:** [2508.14090](https://arxiv.org/abs/2508.14090)  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 8 æœˆ 26 æ—¥  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**æå‡ºä¸“é—¨é’ˆå¯¹ DLM çš„é«˜æ•ˆ Post-Training Quantization æ¡†æ¶**

**é—®é¢˜ï¼š**
- ç°æœ‰ PTQ æ–¹æ³•ï¼ˆå¦‚ AWQï¼‰ç›´æ¥åº”ç”¨åˆ° DLM æ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™
- DLM çš„å¤šæ­¥è¿­ä»£æ¨ç†å¯¼è‡´è¯¯å·®ç´¯ç§¯

**è§£å†³æ–¹æ¡ˆï¼š**
- Timestep-Aware æ ¡å‡†
- è¯¯å·®è¡¥å¿æœºåˆ¶
- æ— éœ€å¾®è°ƒ (Fine-tuning)

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### A. Timestep-Aware æ ¡å‡†

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
DLM æ¨ç†éœ€è¦å¤šæ­¥å»å™ªï¼ˆå¦‚ 50 æ­¥ï¼‰
æ¯ä¸€æ­¥çš„æ¿€æ´»åˆ†å¸ƒä¸åŒ
â†’ éœ€è¦å¯¹æ¯ä¸ª timestep å•ç‹¬æ ¡å‡†
```

**ç®—æ³•ï¼š**
```python
def dllm_quantize(model, calibration_data, n_timesteps=50):
    """
    DLLMQuant: Timestep-Aware æ ¡å‡†
    """
    # 1. å¯¹æ¯ä¸ª timestep æ”¶é›†æ¿€æ´»ç»Ÿè®¡
    timestep_stats = []
    for t in range(n_timesteps):
        stats = collect_activation_stats(model, calibration_data, timestep=t)
        timestep_stats.append(stats)
    
    # 2. èšåˆç»Ÿè®¡ï¼ˆåŠ æƒå¹³å‡ï¼‰
    aggregated_stats = aggregate_stats(timestep_stats, weights='uniform')
    
    # 3. åŸºäºèšåˆç»Ÿè®¡è®¡ç®—é‡åŒ–å‚æ•°
    quant_params = compute_quant_params(aggregated_stats)
    
    # 4. åº”ç”¨é‡åŒ–
    model = apply_quantization(model, quant_params)
    
    return model
```

**æƒé‡ç­–ç•¥ï¼š**
- Uniform: æ‰€æœ‰ timestep æƒé‡ç›¸åŒ
- Early-weighted: æ—©æœŸ timestep æƒé‡æ›´é«˜ï¼ˆå¤„ç†å…¨å±€ç»“æ„ï¼‰
- Late-weighted: åæœŸ timestep æƒé‡æ›´é«˜ï¼ˆå¤„ç†å±€éƒ¨ç»†èŠ‚ï¼‰

**å®éªŒå‘ç°ï¼š**
- Uniform æƒé‡åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½
- Early-weighted åœ¨ç”Ÿæˆè´¨é‡æ•æ„Ÿä»»åŠ¡ä¸Šç•¥å¥½

---

#### B. è¯¯å·®è¡¥å¿æœºåˆ¶

**é—®é¢˜ï¼š**
- é‡åŒ–è¯¯å·®åœ¨å¤šæ­¥æ¨ç†ä¸­ç´¯ç§¯
- å¯¼è‡´æœ€ç»ˆè¾“å‡ºè´¨é‡ä¸‹é™

**è§£å†³æ–¹æ¡ˆï¼š**
```
Step 1: é‡åŒ–æƒé‡ W â†’ W_q
Step 2: è®¡ç®—é‡åŒ–è¯¯å·® E = W - W_q
Step 3: åœ¨æ¨ç†æ—¶è¡¥å¿ï¼šoutput = f(x, W_q) + g(x, E)
```

**å®ç°ï¼š**
```python
class QuantizedLinearWithCompensation(nn.Module):
    def __init__(self, linear_layer, bits=4):
        super().__init__()
        self.original_weight = linear_layer.weight.data.clone()
        self.quantized_weight = quantize(self.original_weight, bits)
        self.error = self.original_weight - self.quantized_weight
        
        # ä½ç§©è¿‘ä¼¼è¯¯å·®ï¼ˆå‡å°‘å­˜å‚¨ï¼‰
        self.error_low_rank = low_rank_approx(self.error, rank=16)
    
    def forward(self, x):
        # ä¸»è¦è®¡ç®—ï¼ˆé‡åŒ–æƒé‡ï¼‰
        out = F.linear(x, self.quantized_weight)
        
        # è¯¯å·®è¡¥å¿ï¼ˆä½ç§©ï¼‰
        compensation = F.linear(x, self.error_low_rank)
        
        return out + compensation
```

**æ•ˆæœï¼š**
- è¡¥å¿å INT4 æ€§èƒ½æ¥è¿‘ FP16
- é¢å¤–è®¡ç®—å¼€é”€ < 5%

---

### ğŸ“Š å®éªŒç»“æœ

#### LLADA-8B é‡åŒ–å¯¹æ¯”

| æ–¹æ³• | æ¯”ç‰¹ | PIQA (MSE) | ç›¸å¯¹ FP16 |
|------|------|------------|-----------|
| **FP16** | 16 | 0.000 | baseline |
| AWQ | INT4 | 0.152 | -18.5% |
| AWQ | INT8 | 0.045 | -4.2% |
| **DLLMQuant** | INT4 | 0.068 | -7.8% |
| **DLLMQuant** | INT8 | 0.012 | -1.1% |

**å…³é”®å‘ç°ï¼š**
- DLLMQuant INT4 æ¯” AWQ INT4 å¥½ 10.7%
- DLLMQuant INT8 å‡ ä¹æ— æŸï¼ˆ-1.1%ï¼‰

---

## 3ï¸âƒ£ Quant-dLLM: æä½æ¯”ç‰¹é‡åŒ–

**ğŸ“„ arXiv:** [2510.03274](https://arxiv.org/abs/2510.03274)  
**ğŸ“… å‘å¸ƒ:** 2025 å¹´ 9 æœˆ 27 æ—¥  
**ğŸ’» ä»£ç :** ï¼ˆå¾…å¼€æºï¼‰

---

### ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**å®ç° DLM çš„ Extreme Low-Bit é‡åŒ–ï¼ˆINT2/INT3ï¼‰**

**æŒ‘æˆ˜ï¼š**
- INT4 ä»¥ä¸‹é‡åŒ–åœ¨ AR LLM ä¸Šå·²ç»éå¸¸å›°éš¾
- DLM çš„å¤šæ­¥æ¨ç†ä½¿é—®é¢˜æ›´å¤æ‚

**åˆ›æ–°ï¼š**
- åˆ†ç»„é‡åŒ– (Group-wise Quantization)
- æ··åˆç²¾åº¦ç­–ç•¥
- æ— éœ€è®­ç»ƒæˆ–åå‘ä¼ æ’­

---

### ğŸ”¬ æ–¹æ³•ç»†èŠ‚

#### A. åˆ†ç»„é‡åŒ– (Group-wise Quantization)

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
ä¼ ç»Ÿé‡åŒ–ï¼šå¯¹æ•´ä¸ªæƒé‡çŸ©é˜µä½¿ç”¨ç»Ÿä¸€çš„ scale
åˆ†ç»„é‡åŒ–ï¼šå°†æƒé‡åˆ†æˆå°ç»„ï¼Œæ¯ç»„ç‹¬ç«‹ scale

ä¼˜åŠ¿ï¼š
- æ›´å¥½åœ°å¤„ç† outliers
- æ¯ç»„å¯ä»¥é€‚åº”ä¸åŒçš„åˆ†å¸ƒ
```

**å®ç°ï¼š**
```python
def groupwise_quantize(W, group_size=128, bits=2):
    """
    åˆ†ç»„é‡åŒ–
    """
    W_q = torch.zeros_like(W)
    scales = []
    zeros = []
    
    # æŒ‰åˆ—åˆ†ç»„
    for i in range(0, W.shape[1], group_size):
        W_group = W[:, i:i+group_size]
        
        # è®¡ç®— per-group scale å’Œ zero point
        scale = W_group.abs().max() / (2 ** (bits - 1) - 1)
        zero_point = (W_group.mean() / scale).round()
        
        # é‡åŒ–
        W_q_group = (W_group / scale).round() + zero_point
        W_q_group = W_q_group.clamp(-2 ** (bits - 1), 2 ** (bits - 1) - 1)
        
        W_q[:, i:i+group_size] = W_q_group
        scales.append(scale)
        zeros.append(zero_point)
    
    return W_q, scales, zeros
```

**ç»„å¤§å°é€‰æ‹©ï¼š**
- group_size=128: æœ€ä½³å¹³è¡¡ï¼ˆç²¾åº¦ vs å¼€é”€ï¼‰
- group_size=64: ç²¾åº¦ç•¥å¥½ï¼Œå­˜å‚¨å¼€é”€å¤§
- group_size=256: å­˜å‚¨æ›´çœï¼Œç²¾åº¦ç•¥é™

---

#### B. æ··åˆç²¾åº¦ç­–ç•¥

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
ä¸åŒå±‚å¯¹é‡åŒ–çš„æ•æ„Ÿåº¦ä¸åŒ
â†’ æ•æ„Ÿå±‚ç”¨é«˜ç²¾åº¦ï¼Œä¸æ•æ„Ÿå±‚ç”¨ä½ç²¾åº¦
```

**æ•æ„Ÿåº¦åˆ†æï¼š**
```python
def analyze_layer_sensitivity(model, calibration_data):
    """
    åˆ†ææ¯å±‚å¯¹é‡åŒ–çš„æ•æ„Ÿåº¦
    """
    sensitivity = {}
    
    for layer_idx, layer in enumerate(model.layers):
        # 1. é‡åŒ–è¯¥å±‚
        layer_q = quantize_layer(layer, bits=2)
        
        # 2. è®¡ç®—è¾“å‡ºå·®å¼‚
        output_diff = compute_output_difference(model, layer_q, calibration_data)
        
        # 3. è®°å½•æ•æ„Ÿåº¦
        sensitivity[layer_idx] = output_diff
    
    return sensitivity

# æ ¹æ®æ•æ„Ÿåº¦åˆ†é…ç²¾åº¦
def assign_mixed_precision(sensitivity, budget=4.0):
    """
    åŸºäºæ•æ„Ÿåº¦åˆ†é…æ··åˆç²¾åº¦
    budget: å¹³å‡æ¯”ç‰¹æ•°
    """
    # æ•æ„Ÿåº¦é«˜çš„å±‚ç”¨ INT8ï¼Œä½çš„ç”¨ INT2
    # ä½¿å¾—å¹³å‡æ¯”ç‰¹æ•°æ¥è¿‘ budget
    ...
```

**å…¸å‹é…ç½®ï¼š**
```
Layer 1-4 (Embedding):  INT8  â† æ•æ„Ÿ
Layer 5-20 (Middle):    INT2  â† ä¸æ•æ„Ÿ
Layer 21-24 (Output):   INT4  â† ä¸­ç­‰
Layer 25-28 (LM Head):  INT8  â† æ•æ„Ÿ

å¹³å‡æ¯”ç‰¹æ•°ï¼š~3.2 bits
```

---

### ğŸ“Š å®éªŒç»“æœ

#### LLaDA-8B æä½æ¯”ç‰¹é‡åŒ–

| æ–¹æ³• | å¹³å‡æ¯”ç‰¹ | MMLU | GSM8K | å‹ç¼©ç‡ |
|------|---------|------|-------|--------|
| **FP16** | 16 | 68.5 | 52.3 | 1x |
| GPTQ | 4 | 65.2 | 42.1 | 4x |
| **Quant-dLLM** | 3.2 | 62.8 | 38.5 | 5x |
| **Quant-dLLM** | 2.5 | 58.2 | 32.1 | 6.4x |
| **Quant-dLLM** | 2.0 | 52.5 | 25.8 | 8x |

**å…³é”®å‘ç°ï¼š**
- INT3.2 æ··åˆç²¾åº¦ï¼šæ€§èƒ½æ¥è¿‘ INT4ï¼Œå‹ç¼©ç‡æ›´é«˜
- INT2 ä»ç„¶å¯ç”¨ï¼ˆ52.5% MMLUï¼‰ï¼Œé€‚åˆæç«¯èµ„æºå—é™åœºæ™¯

---

## ğŸ“ˆ æŠ€æœ¯å¯¹æ¯”æ€»ç»“

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | æ ¸å¿ƒåˆ›æ–° | æœ€ä½æ¯”ç‰¹ | æ— éœ€è®­ç»ƒ | ä»£ç å¼€æº |
|------|---------|---------|---------|---------|
| **QDLM** | ç³»ç»Ÿæ€§è¯„ä¼° | INT4 | âœ… | âœ… |
| **DLLMQuant** | Timestep-Aware | INT4 | âœ… | âŒ |
| **Quant-dLLM** | åˆ†ç»„ + æ··åˆç²¾åº¦ | INT2 | âœ… | âŒ |

---

### æ¨èé…ç½®

| åœºæ™¯ | æ¨èæ–¹æ³• | æ¯”ç‰¹æ•° | ç†ç”± |
|------|---------|--------|------|
| **é€šç”¨éƒ¨ç½²** | QDLM (GPTQ) | INT4 | æˆç†Ÿï¼Œå¼€æºï¼Œå¹³è¡¡ |
| **é«˜è´¨é‡è¦æ±‚** | DLLMQuant | INT8 | æ¥è¿‘æ— æŸ |
| **æç«¯å‹ç¼©** | Quant-dLLM | INT2-3 | æœ€é«˜å‹ç¼©ç‡ |
| **æ•°å­¦/ä»£ç ** | QDLM (GPTQ) | INT8 | é«˜æ•æ„Ÿä»»åŠ¡ |
| **è¾¹ç¼˜è®¾å¤‡** | Quant-dLLM | INT3.2 | å‹ç¼©ç‡ä¼˜å…ˆ |

---

## ğŸ” ç›¸å…³æŠ€æœ¯å’Œå·¥å…·

### AR LLM é‡åŒ–æ–¹æ³•ï¼ˆå¯å€Ÿé‰´ï¼‰

| æ–¹æ³• | ç±»å‹ | é“¾æ¥ |
|------|------|------|
| **GPTQ** | Weight-only | [arXiv:2210.17323](https://arxiv.org/abs/2210.17323) |
| **AWQ** | Weight-only | [arXiv:2306.00978](https://arxiv.org/abs/2306.00978) |
| **SmoothQuant** | W-A | [arXiv:2211.10438](https://arxiv.org/abs/2211.10438) |
| **QuaRot** | W-A | [arXiv:2404.00456](https://arxiv.org/abs/2404.00456) |
| **DuQuant** | W-A | [arXiv:2404.04809](https://arxiv.org/abs/2404.04809) |
| **LLM.int8()** | W-A | [arXiv:2208.07339](https://arxiv.org/abs/2208.07339) |
| **QLoRA** | Finetuning | [arXiv:2305.14314](https://arxiv.org/abs/2305.14314) |

---

### å·¥å…·åº“

| å·¥å…· | åŠŸèƒ½ | é“¾æ¥ |
|------|------|------|
| **bitsandbytes** | INT8/FP4 é‡åŒ– | [GitHub](https://github.com/TimDettmers/bitsandbytes) |
| **AutoGPTQ** | GPTQ å®ç° | [GitHub](https://github.com/AutoGPTQ/AutoGPTQ) |
| **LLM-AWQ** | AWQ å®ç° | [GitHub](https://github.com/mit-han-lab/llm-awq) |
| **HuggingFace Optimum** | é‡åŒ–å·¥å…·é›† | [Docs](https://huggingface.co/docs/optimum) |

---

## ğŸ¯ å¼€æ”¾é—®é¢˜ï¼ˆç ”ç©¶æœºä¼šï¼‰

### é«˜ä¼˜å…ˆçº§

#### 1. Timestep-Adaptive Quantization

**é—®é¢˜ï¼š** ç°æœ‰æ–¹æ³•å¯¹æ‰€æœ‰ timestep ä½¿ç”¨ç›¸åŒé‡åŒ–ç­–ç•¥

**æœºä¼šï¼š**
```
æ—©æœŸ timestep (é«˜å™ªå£°): éœ€è¦é«˜ç²¾åº¦ï¼ˆå…¨å±€ç»“æ„ï¼‰
åæœŸ timestep (ä½å™ªå£°): å¯ç”¨ä½ç²¾åº¦ï¼ˆå±€éƒ¨ç»†èŠ‚ï¼‰

â†’ åŠ¨æ€è°ƒæ•´æ¯æ­¥çš„é‡åŒ–ç²¾åº¦
```

**æ½œåœ¨æ”¶ç›Šï¼š**
- ç›¸åŒè´¨é‡ä¸‹ï¼Œå¹³å‡æ¯”ç‰¹æ•°é™ä½ 20-30%
- æˆ–ç›¸åŒæ¯”ç‰¹æ•°ä¸‹ï¼Œè´¨é‡æå‡

---

#### 2. Joint Pruning + Quantization

**é—®é¢˜ï¼š** å‰ªæå’Œé‡åŒ–é€šå¸¸åˆ†å¼€åš

**æœºä¼šï¼š**
```
åŒæ—¶ä¼˜åŒ–ï¼š
- å“ªäº›æƒé‡å¯ä»¥å‰ªæ‰ï¼Ÿ
- å“ªäº›æƒé‡éœ€è¦é«˜ç²¾åº¦ï¼Ÿ
- å“ªäº›æƒé‡å¯ä»¥ç”¨ä½ç²¾åº¦ï¼Ÿ

â†’ æ‰¾åˆ°æœ€ä¼˜çš„ quality-efficiency frontier
```

**å‚è€ƒï¼š** Sink-Aware Pruning + DLLMQuant ç»“åˆ

---

#### 3. Quantization-Aware Training for DLMs

**é—®é¢˜ï¼š** ç°æœ‰æ–¹æ³•éƒ½æ˜¯ PTQï¼ˆPost-Trainingï¼‰

**æœºä¼šï¼š**
```
åœ¨ DLM é¢„è®­ç»ƒæˆ–å¾®è°ƒæ—¶åŠ å…¥é‡åŒ–æ„ŸçŸ¥
â†’ æ›´å¥½çš„ä½æ¯”ç‰¹æ€§èƒ½

æŒ‘æˆ˜ï¼š
- DLM è®­ç»ƒæˆæœ¬å·²ç»å¾ˆé«˜
- éœ€è¦é«˜æ•ˆçš„ QAT æ–¹æ³•
```

---

#### 4. Hardware-Aware Optimization

**é—®é¢˜ï¼š** ç°æœ‰æ–¹æ³•ä¸è€ƒè™‘ç›®æ ‡ç¡¬ä»¶ç‰¹æ€§

**æœºä¼šï¼š**
```
é’ˆå¯¹ä¸åŒç¡¬ä»¶ä¼˜åŒ–ï¼š
- NVIDIA GPU (Tensor Core)
- AMD GPU
- Edge TPU
- Mobile NPU

â†’ å®é™…éƒ¨ç½²æ—¶æ€§èƒ½æ›´å¥½
```

---

### ä¸­ä¼˜å…ˆçº§

#### 5. Activation Quantization for DLMs

**ç°çŠ¶ï¼š** å¤§å¤šæ•°å·¥ä½œåªåšæƒé‡é‡åŒ–

**æœºä¼šï¼š**
- æ¿€æ´»é‡åŒ–å¯ä»¥è¿›ä¸€æ­¥åŠ é€Ÿï¼ˆæ•´æ•°çŸ©é˜µä¹˜æ³•ï¼‰
- ä½† DLM çš„æ¿€æ´» outliers æ›´å¤æ‚
- éœ€è¦æ–°çš„æ¿€æ´»é‡åŒ–æ–¹æ³•

---

#### 6. Long-Context DLM Quantization

**é—®é¢˜ï¼š** é•¿åºåˆ—ä¸‹ KV cache æˆä¸ºç“¶é¢ˆ

**æœºä¼šï¼š**
- KV cache é‡åŒ–
- ç¨€ç–æ³¨æ„åŠ› + é‡åŒ–
- é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡çš„ç‰¹æ®Šä¼˜åŒ–

---

## ğŸ“š æ¨èé˜…è¯»é¡ºåº

### å…¥é—¨ï¼ˆäº†è§£é¢†åŸŸï¼‰
1. **Quantization Meets dLLMs** â€” ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œå¿…è¯»
2. **A Survey on Diffusion Language Models** â€” DLM æ•´ä½“ survey

### è¿›é˜¶ï¼ˆæŠ€æœ¯ç»†èŠ‚ï¼‰
3. **DLLMQuant** â€” Timestep-Aware æ ¡å‡†
4. **Quant-dLLM** â€” æä½æ¯”ç‰¹é‡åŒ–

### æ‹“å±•ï¼ˆAR LLM é‡åŒ–ï¼‰
5. **GPTQ** â€” ç»å…¸æƒé‡é‡åŒ–
6. **AWQ** â€” æ¿€æ´»æ„ŸçŸ¥é‡åŒ–
7. **SmoothQuant** â€” æƒé‡ - æ¿€æ´»é‡åŒ–

---

## ğŸ¯ å¯¹ä½ çš„ç ”ç©¶å»ºè®®

### å¦‚æœåš DLM é‡åŒ–

**çŸ­æœŸï¼ˆ1-2 æœˆï¼‰ï¼š**
1. å¤ç° QDLM (GPTQ) åœ¨ LLaDA ä¸Š
2. éªŒè¯ activation outliers ç°è±¡
3. å°è¯• Timestep-Adaptive é‡åŒ–

**ä¸­æœŸï¼ˆ3-6 æœˆï¼‰ï¼š**
1. å®ç° Joint Pruning + Quantization
2. åœ¨å¤šä¸ª DLM ä¸ŠéªŒè¯
3. å†™è®ºæ–‡ï¼ˆç›®æ ‡ï¼šICLR/NeurIPSï¼‰

**é•¿æœŸï¼ˆ6-12 æœˆï¼‰ï¼š**
1. æ¢ç´¢ QAT for DLMs
2. Hardware-Aware ä¼˜åŒ–
3. å¼€æºå·¥å…·ï¼Œå»ºç«‹å½±å“åŠ›

---

### å¦‚æœåšç›¸å…³æ–¹å‘

**å¯å€Ÿé‰´çš„æ€è·¯ï¼š**
- Timestep-Aware â†’ å¯ç”¨äºå…¶ä»– DLM ä¼˜åŒ–
- Group-wise Quantization â†’ é€šç”¨æŠ€æœ¯
- Mixed-Precision â†’ ç³»ç»Ÿçº§ä¼˜åŒ–

---

## ğŸ“¬ æ€»ç»“

**é¢†åŸŸç°çŠ¶ï¼š**
- DLM é‡åŒ–ç ”ç©¶åˆšåˆšèµ·æ­¥ï¼ˆ2025 å¹´ 8 æœˆç¬¬ä¸€ç¯‡ç³»ç»Ÿç ”ç©¶ï¼‰
- 3 ç¯‡æ ¸å¿ƒè®ºæ–‡æä¾›äº†åŸºç¡€æ–¹æ³•
- å¤§é‡å¼€æ”¾é—®é¢˜ç­‰å¾…æ¢ç´¢

**æ¨èèµ·ç‚¹ï¼š**
- ä» QDLM (GPTQ) å¼€å§‹
- åœ¨ LLaDA-8B ä¸Šå¤ç°
- é€æ­¥æ¢ç´¢æ”¹è¿›æ–¹å‘

**ç ”ç©¶ä»·å€¼ï¼š**
- DLM æ˜¯æ–°å…´æ–¹å‘ï¼ŒEfficient ä¼˜åŒ–éœ€æ±‚å¤§
- å·¥ä¸šç•Œéœ€è¦ï¼ˆæ¨ç†æˆæœ¬å¤ªé«˜ï¼‰
- å­¦æœ¯ä»·å€¼é«˜ï¼ˆé¡¶ä¼šå‹å¥½ï¼‰

---

*è¿”å› [00-daily-updates.md](00-daily-updates.md)*
